% ============================================================================% ============================================================================

% MASTER REFERENCES FILE% 1. INTRODUCTION

% ============================================================================% ============================================================================

% This file organizes all references by paper section for easy citation management.% Use these to motivate "RL in complex stochastic games", "Yahtzee as a hard 

%% combinatorial domain," and "deep RL works for big games, so let's try it here."

% USAGE IN YOUR LATEX FILE:

% Add this line in your preamble or where you normally include bibliography:% ----------------------------------------------------------------------------

% \bibliography{references}% 1.1 RL in Games / Long-Horizon Control

%% ----------------------------------------------------------------------------

% Or if you want to see the section organization preserved, you can include 

% individual files using a bibliography management tool, or just use this % Classic case study of TD learning beating humans in a highly stochastic board game (backgammon).

% master file which includes everything organized by section.% Cite when motivating self-play + value function approximation in stochastic environments.

%@article{tesauro1995tdgammon,

% ORGANIZATION:  title   = {Temporal Difference Learning and TD-Gammon},

% - Section 1: Introduction  author  = {Tesauro, Gerald},

% - Section 2: Related Work    journal = {Communications of the ACM},

% - Section 3: Methods  volume  = {38},

% - Section 4: Single-Turn Results (mostly back-references)  number  = {3},

% - Section 5: Full-Game Experiments (mostly back-references)  pages   = {58--68},

% - Section 6: Discussion (mostly back-references)  year    = {1995},

% - Section 7: Appendix (mostly back-references)  doi     = {10.1145/203330.203343}

%}

% NOTE: Some citations appear in multiple sections. To avoid duplicates,

% references are defined only once in their "home" section and later sections% Original AlphaGo paper: deep nets + MCTS, human data + self-play.

% include comments indicating where to find them.% Cite when introducing deep RL + tree search for board games or "superhuman Go with human data".

% ============================================================================@article{silver2016alphago,

  title   = {Mastering the Game of Go with Deep Neural Networks and Tree Search},

  author  = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},

% ============================================================================  journal = {Nature},

% 1. INTRODUCTION  volume  = {529},

% ============================================================================  number  = {7587},

% Use these to motivate "RL in complex stochastic games", "Yahtzee as a hard   pages   = {484--489},

% combinatorial domain," and "deep RL works for big games, so let's try it here."  year    = {2016},

  doi     = {10.1038/nature16961}

% ----------------------------------------------------------------------------}

% 1.1 RL in Games / Long-Horizon Control

% ----------------------------------------------------------------------------% AlphaGo Zero: no human data, pure self-play, massive compute.

% Cite when emphasizing "learning from scratch" and strong self-play training regimes.

% Classic case study of TD learning beating humans in a highly stochastic board game (backgammon).@article{silver2017alphagozero,

% Cite when motivating self-play + value function approximation in stochastic environments.  title   = {Mastering the Game of Go without Human Knowledge},

@article{tesauro1995tdgammon,  author  = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and others},

  title   = {Temporal Difference Learning and TD-Gammon},  journal = {Nature},

  author  = {Tesauro, Gerald},  volume  = {550},

  journal = {Communications of the ACM},  number  = {7676},

  volume  = {38},  pages   = {354--359},

  number  = {3},  year    = {2017},

  pages   = {58--68},  doi     = {10.1038/nature24270}

  year    = {1995},}

  doi     = {10.1145/203330.203343}

}% AlphaZero: single general algorithm for Go/Chess/Shogi via self-play.

% Cite when framing your method as "AlphaZero-style self-play for game X" or arguing for generality.

% Original AlphaGo paper: deep nets + MCTS, human data + self-play.@article{silver2018alphazero,

% Cite when introducing deep RL + tree search for board games or "superhuman Go with human data".  title   = {A General Reinforcement Learning Algorithm that Masters Chess, Shogi, and Go through Self-Play},

@article{silver2016alphago,  author  = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},

  title   = {Mastering the Game of Go with Deep Neural Networks and Tree Search},  journal = {Science},

  author  = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},  volume  = {362},

  journal = {Nature},  number  = {6419},

  volume  = {529},  pages   = {1140--1144},

  number  = {7587},  year    = {2018},

  pages   = {484--489},  doi     = {10.1126/science.aar6404}

  year    = {2016},}

  doi     = {10.1038/nature16961}

}% DeepStack poker AI.

% Cite when discussing RL + search + value networks in high-variance, imperfect-information games (No-Limit Texas Hold'em).

% AlphaGo Zero: no human data, pure self-play, massive compute.@article{moravcik2017deepstack,

% Cite when emphasizing "learning from scratch" and strong self-play training regimes.  title   = {DeepStack: Expert-Level Artificial Intelligence in Heads-Up No-Limit Poker},

@article{silver2017alphagozero,  author  = {Morav{\v{c}}{\'\i}k, Matej and Schmid, Martin and Burch, Neil and Lis{\'y}, Viliam and Morrill, Dustin and Bard, Nolan and Davis, Trevor and Waugh, Kevin and Johanson, Michael and Bowling, Michael},

  title   = {Mastering the Game of Go without Human Knowledge},  journal = {Science},

  author  = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and others},  volume  = {356},

  journal = {Nature},  number  = {6337},

  volume  = {550},  pages   = {508--513},

  number  = {7676},  year    = {2017},

  pages   = {354--359},  doi     = {10.1126/science.aam6960}

  year    = {2017},}

  doi     = {10.1038/nature24270}

}% Libratus poker AI.

% Cite when referencing game-theoretic RL / equilibrium-finding in complex stochastic games vs. human pros.

% AlphaZero: single general algorithm for Go/Chess/Shogi via self-play.@article{brown2018libratus,

% Cite when framing your method as "AlphaZero-style self-play for game X" or arguing for generality.  title   = {Superhuman {AI} for Heads-Up No-Limit Poker: {Libratus} Beats Top Professionals},

@article{silver2018alphazero,  author  = {Brown, Noam and Sandholm, Tuomas},

  title   = {A General Reinforcement Learning Algorithm that Masters Chess, Shogi, and Go through Self-Play},  journal = {Science},

  author  = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},  volume  = {359},

  journal = {Science},  number  = {6374},

  volume  = {362},  pages   = {418--424},

  number  = {6419},  year    = {2018},

  pages   = {1140--1144},  doi     = {10.1126/science.aao1733}

  year    = {2018},}

  doi     = {10.1126/science.aar6404}

}% Hanabi challenge paper.

% Cite when motivating cooperative multi-agent RL, partial observability, or "hard stochastic card games".

% DeepStack poker AI.@article{bard2020hanabi,

% Cite when discussing RL + search + value networks in high-variance, imperfect-information games (No-Limit Texas Hold'em).  title   = {The Hanabi Challenge: A New Frontier for {AI} Research},

@article{moravcik2017deepstack,  author  = {Bard, Nolan and Foerster, Jakob N. and Chandar, Sarath and Burch, Neil and Lanctot, Marc and Song, H. Francis and Parisotto, Emilio and Dumoulin, Vincent and Moitra, Subhodeep and Hughes, Edward and Dunning, Iain and Mourad, Shibl and Larochelle, Hugo and Bellemare, Marc G. and Bowling, Michael},

  title   = {DeepStack: Expert-Level Artificial Intelligence in Heads-Up No-Limit Poker},  journal = {Artificial Intelligence},

  author  = {Morav{\v{c}}{\'\i}k, Matej and Schmid, Martin and Burch, Neil and Lis{\'y}, Viliam and Morrill, Dustin and Bard, Nolan and Davis, Trevor and Waugh, Kevin and Johanson, Michael and Bowling, Michael},  volume  = {280},

  journal = {Science},  pages   = {103216},

  volume  = {356},  year    = {2020},

  number  = {6337},  doi     = {10.1016/j.artint.2019.103216}

  pages   = {508--513},}

  year    = {2017},

  doi     = {10.1126/science.aam6960}% Early ADP/TD work on Tetris.

}% Cite when talking about approximate dynamic programming on huge, stochastic MDPs (e.g., Tetris as a benchmark for instability/difficulty).

@techreport{bertsekas1996tetris,

% Libratus poker AI.  title       = {Temporal Differences-Based Policy Iteration and Applications in Neuro-Dynamic Programming},

% Cite when referencing game-theoretic RL / equilibrium-finding in complex stochastic games vs. human pros.  author      = {Bertsekas, Dimitri P. and Ioffe, Sergey},

@article{brown2018libratus,  institution = {Laboratory for Information and Decision Systems, MIT},

  title   = {Superhuman {AI} for Heads-Up No-Limit Poker: {Libratus} Beats Top Professionals},  number      = {LIDS-P-2349},

  author  = {Brown, Noam and Sandholm, Tuomas},  year        = {1996}

  journal = {Science},}

  volume  = {359},

  number  = {6374},% "Tetris is actually hard but ADP can work" paper.

  pages   = {418--424},% Cite when you want a modern-ish RL/ADP reference on stochastic combinatorial games beyond Go/Atari (e.g., as evidence RL can crack tough puzzle-like domains).

  year    = {2018},@inproceedings{gabillon2013tetris,

  doi     = {10.1126/science.aao1733}  title     = {Approximate Dynamic Programming Finally Performs Well in the Game of Tetris},

}  author    = {Gabillon, Victor and Ghavamzadeh, Mohammad and Lazaric, Alessandro and Scherrer, Bruno},

  booktitle = {Advances in Neural Information Processing Systems},

% Hanabi challenge paper.  volume    = {26},

% Cite when motivating cooperative multi-agent RL, partial observability, or "hard stochastic card games".  year      = {2013}

@article{bard2020hanabi,}

  title   = {The Hanabi Challenge: A New Frontier for {AI} Research},

  author  = {Bard, Nolan and Foerster, Jakob N. and Chandar, Sarath and Burch, Neil and Lanctot, Marc and Song, H. Francis and Parisotto, Emilio and Dumoulin, Vincent and Moitra, Subhodeep and Hughes, Edward and Dunning, Iain and Mourad, Shibl and Larochelle, Hugo and Bellemare, Marc G. and Bowling, Michael},% Bootstrapped DQN for deep exploration.

  journal = {Artificial Intelligence},% Cite when you need a canonical "exploration in deep RL" reference or when arguing about handling high stochasticity via better exploration.

  volume  = {280},@inproceedings{osband2016bootstrappeddqn,

  pages   = {103216},  title     = {Deep Exploration via Bootstrapped {DQN}},

  year    = {2020},  author    = {Osband, Ian and Blundell, Charles and Pritzel, Alexander and Van Roy, Benjamin},

  doi     = {10.1016/j.artint.2019.103216}  booktitle = {Advances in Neural Information Processing Systems},

}  volume    = {29},

  year      = {2016}

% Early ADP/TD work on Tetris.}

% Cite when talking about approximate dynamic programming on huge, stochastic MDPs (e.g., Tetris as a benchmark for instability/difficulty).

@techreport{bertsekas1996tetris,% ----------------------------------------------------------------------------

  title       = {Temporal Differences-Based Policy Iteration and Applications in Neuro-Dynamic Programming},% 1.2 Yahtzee / This Domain Is Hard

  author      = {Bertsekas, Dimitri P. and Ioffe, Sergey},% ----------------------------------------------------------------------------

  institution = {Laboratory for Information and Decision Systems, MIT},

  number      = {LIDS-P-2349},@techreport{glenn-2006-optimal-yahtzee,

  year        = {1996}  author      = {Jeffrey R. Glenn},

}  title       = {An Optimal Strategy for Yahtzee},

  institution = {Loyola College in Maryland, Department of Computer Science},

% "Tetris is actually hard but ADP can work" paper.  number      = {CS-TR-0002},

% Cite when you want a modern-ish RL/ADP reference on stochastic combinatorial games beyond Go/Atari (e.g., as evidence RL can crack tough puzzle-like domains).  year        = {2006},

@inproceedings{gabillon2013tetris,  url         = {https://gunpowder.cs.loyola.edu/~jglenn/research/optimal_yahtzee.pdf}

  title     = {Approximate Dynamic Programming Finally Performs Well in the Game of Tetris},}

  author    = {Gabillon, Victor and Ghavamzadeh, Mohammad and Lazaric, Alessandro and Scherrer, Bruno},

  booktitle = {Advances in Neural Information Processing Systems},@inproceedings{glenn-2007-solitaire-yahtzee,

  volume    = {26},  author    = {Jeffrey R. Glenn},

  year      = {2013}  title     = {Computer Strategies for Solitaire Yahtzee},

}  booktitle = {2007 IEEE Symposium on Computational Intelligence and Games (CIG)},

  year      = {2007},

% Bootstrapped DQN for deep exploration.  pages     = {132--139},

% Cite when you need a canonical "exploration in deep RL" reference or when arguing about handling high stochasticity via better exploration.  doi       = {10.1109/CIG.2007.368085}

@inproceedings{osband2016bootstrappeddqn,}

  title     = {Deep Exploration via Bootstrapped {DQN}},

  author    = {Osband, Ian and Blundell, Charles and Pritzel, Alexander and Van Roy, Benjamin},@misc{verhoeff-1999-solitaire-yahtzee,

  booktitle = {Advances in Neural Information Processing Systems},  author       = {Tom Verhoeff},

  volume    = {29},  title        = {Optimal Solitaire Yahtzee Strategies (slides)},

  year      = {2016}  year         = {1999},

}  howpublished = {\url{https://www-set.win.tue.nl/~wstomv/misc/yahtzee/slides-2up.pdf}}

}

% ----------------------------------------------------------------------------

% 1.2 Yahtzee / This Domain Is Hard% Multiplayer Yahtzee

% ----------------------------------------------------------------------------@inproceedings{pawlewicz-2011-multiplayer-yahtzee,

  author    = {Pawlewicz, Jakub},

% DP optimal strategy for Yahtzee  editor    = {van den Herik, H. Jaap

@techreport{glenn-2006-optimal-yahtzee,               and Iida, Hiroyuki

  author      = {Jeffrey R. Glenn},               and Plaat, Aske},

  title       = {An Optimal Strategy for Yahtzee},  title     = {Nearly Optimal Computer Play in Multi-player Yahtzee},

  institution = {Loyola College in Maryland, Department of Computer Science},  booktitle = {Computers and Games},

  number      = {CS-TR-0002},  year      = {2011},

  year        = {2006},  publisher = {Springer Berlin Heidelberg},

  url         = {https://gunpowder.cs.loyola.edu/~jglenn/research/optimal_yahtzee.pdf}  address   = {Berlin, Heidelberg},

}  pages     = {250--262},

  abstract  = {Yahtzee is the most popular commercial dice game in the world. It can be played either by one or many players. In case of the single-player version, optimal computer strategies both for maximizing the expected average score and for maximizing the probability of beating a particular score are already known. However, when it comes to the multi-player version, those approaches are far too resource intensive and thus are not able to develop an optimal strategy given the current hardware.},

@inproceedings{glenn-2007-solitaire-yahtzee,  isbn      = {978-3-642-17928-0}

  author    = {Jeffrey R. Glenn},}

  title     = {Computer Strategies for Solitaire Yahtzee},

  booktitle = {2007 IEEE Symposium on Computational Intelligence and Games (CIG)},@manual{hasbro-2022-yahtzee-rules,

  year      = {2007},  title        = {YAHTZEE Game: Instructions},

  pages     = {132--139},  author       = {{Hasbro, Inc.}},

  doi       = {10.1109/CIG.2007.368085}  year         = {2022},

}  note         = {Official rules and instructions},

  howpublished = {\url{https://instructions.hasbro.com/en-nz/instruction/yahtzee-game}}

@misc{verhoeff-1999-solitaire-yahtzee,}

  author       = {Tom Verhoeff},

  title        = {Optimal Solitaire Yahtzee Strategies (slides)},% ----------------------------------------------------------------------------

  year         = {1999},% 1.3 Capacity / Network Size Motivation (optional but spicy)

  howpublished = {\url{https://www-set.win.tue.nl/~wstomv/misc/yahtzee/slides-2up.pdf}}% ----------------------------------------------------------------------------

}

% Talks about minimum and lower bounds on neural network size given game space.

% Multiplayer Yahtzee complexity@inproceedings{horne-1994-bounds-rnn-fsm,

@inproceedings{pawlewicz-2011-multiplayer-yahtzee,  author    = {Horne, Bill G. and Hush, Don R.},

  author    = {Pawlewicz, Jakub},  title     = {Bounds on the Complexity of Recurrent Neural Network Implementations of Finite State Machines},

  editor    = {van den Herik, H. Jaap and Iida, Hiroyuki and Plaat, Aske},  booktitle = {Advances in Neural Information Processing Systems 6},

  title     = {Nearly Optimal Computer Play in Multi-player Yahtzee},  editor    = {Cowan, Jack D. and Tesauro, Gerald and Alspector, Joshua},

  booktitle = {Computers and Games},  pages     = {359--366},

  year      = {2011},  year      = {1994},

  publisher = {Springer Berlin Heidelberg},  publisher = {Morgan Kaufmann},

  address   = {Berlin, Heidelberg},  address   = {San Francisco, CA}

  pages     = {250--262},}

  isbn      = {978-3-642-17928-0}

}@article{hanin-2017-bounded-width-relu,

  author       = {Boris Hanin},

% Official game rules  title        = {Universal Function Approximation by Deep Neural Nets with Bounded Width and ReLU Activations},

@manual{hasbro-2022-yahtzee-rules,  year         = {2017},

  title        = {YAHTZEE Game: Instructions},  eprint       = {arXiv:1708.02691},

  author       = {{Hasbro, Inc.}},  howpublished = {Mathematics 2019, 7(10), 992},

  year         = {2022},  doi          = {10.3390/math7100992}

  note         = {Official rules and instructions},}

  howpublished = {\url{https://instructions.hasbro.com/en-nz/instruction/yahtzee-game}}

}

% ============================================================================

% ----------------------------------------------------------------------------% 2. RELATED WORK

% 1.3 Capacity / Network Size Motivation (optional but spicy)% ============================================================================

% ----------------------------------------------------------------------------% This is where you dump most of the Yahtzee-specific and game-RL stuff, 

% plus classic RL theory.

% Talks about minimum and lower bounds on neural network size given game space.

@inproceedings{horne-1994-bounds-rnn-fsm,% ----------------------------------------------------------------------------

  author    = {Horne, Bill G. and Hush, Don R.},% 2.1 Yahtzee and Close Cousins

  title     = {Bounds on the Complexity of Recurrent Neural Network Implementations of Finite State Machines},% ----------------------------------------------------------------------------

  booktitle = {Advances in Neural Information Processing Systems 6},

  editor    = {Cowan, Jack D. and Tesauro, Gerald and Alspector, Joshua},% DP Solvers for Yahtzee (already defined above, can be re-cited here)

  pages     = {359--366},% glenn-2006-optimal-yahtzee

  year      = {1994},% glenn-2007-solitaire-yahtzee

  publisher = {Morgan Kaufmann},% verhoeff-1999-solitaire-yahtzee

  address   = {San Francisco, CA}% pawlewicz-2011-multiplayer-yahtzee

}

% Student / project work:

@article{hanin-2017-bounded-width-relu,@unpublished{kang-2018-yahtzee-rl,

  author       = {Boris Hanin},  author = {Kang, Minhyung and Schroeder, Luca},

  title        = {Universal Function Approximation by Deep Neural Nets with Bounded Width and ReLU Activations},  title  = {Reinforcement Learning for Solving Yahtzee},

  year         = {2017},  year   = {2018},

  eprint       = {arXiv:1708.02691},  note   = {AA228: Decision Making under Uncertainty, Stanford University, class project report},

  howpublished = {Mathematics 2019, 7(10), 992},  url    = {https://web.stanford.edu/class/aa228/reports/2018/final75.pdf}

  doi          = {10.3390/math7100992}}

}

@bachelorsthesis{vasseur-2019-strategy-ladders,

  author  = {Vasseur, Philip},

% ============================================================================  title   = {Using Deep Q-Learning to Compare Strategy Ladders of Yahtzee},

% 2. RELATED WORK  school  = {Yale University},

% ============================================================================  type    = {Undergraduate thesis, Department of Computer Science},

% This is where you dump most of the Yahtzee-specific and game-RL stuff,   address = {New Haven, CT},

% plus classic RL theory.  year    = {2019},

  url     = {https://raw.githubusercontent.com/philvasseur/Yahtzee-DQN-Thesis/dcf2bfe15c3b8c0ff3256f02dd3c0aabdbcbc9bb/webpage/final_report.pdf},

% ----------------------------------------------------------------------------  note    = {Source code available at \url{https://github.com/philvasseur/Yahtzee-DQN-Thesis}}

% 2.1 Yahtzee and Close Cousins}

% ----------------------------------------------------------------------------

@unpublished{yuan-2023-two-player-yahtzee,

% NOTE: glenn-2006-optimal-yahtzee, glenn-2007-solitaire-yahtzee,   author      = {Yuan, Max},

% verhoeff-1999-solitaire-yahtzee, pawlewicz-2011-multiplayer-yahtzee   title       = {Using Deep Q-Learning to Play Two-Player Yahtzee},

% are already defined in Section 1 above and can be re-cited here.  note        = {Senior essay, Computer Science and Economics},

  institution = {Yale University},

% Student / project work:  address     = {New Haven, CT, USA},

@unpublished{kang-2018-yahtzee-rl,  year        = {2023},

  author = {Kang, Minhyung and Schroeder, Luca},  month       = {12},

  title  = {Reinforcement Learning for Solving Yahtzee},  advisor     = {James R. Glenn},

  year   = {2018},  url         = {https://csec.yale.edu/senior-essays/fall-2023/using-deep-q-learning-play-two-player-yahtzee}

  note   = {AA228: Decision Making under Uncertainty, Stanford University, class project report},}

  url    = {https://web.stanford.edu/class/aa228/reports/2018/final75.pdf}

}% Similar efforts

@misc{belaich-2024-yams,

@bachelorsthesis{vasseur-2019-strategy-ladders,  author       = {Belaich, Alae},

  author  = {Vasseur, Philip},  title        = {{YAMS: Reinforcement Learning Project}},

  title   = {Using Deep Q-Learning to Compare Strategy Ladders of Yahtzee},  year         = {2024},

  school  = {Yale University},  howpublished = {\url{https://github.com/abelaich/YAMS-Reinforcement-Learning-Project}},

  type    = {Undergraduate thesis, Department of Computer Science},  note         = {GitHub repository, accessed 2025-11-16}

  address = {New Haven, CT},}

  year    = {2019},

  url     = {https://raw.githubusercontent.com/philvasseur/Yahtzee-DQN-Thesis/dcf2bfe15c3b8c0ff3256f02dd3c0aabdbcbc9bb/webpage/final_report.pdf},% ----------------------------------------------------------------------------

  note    = {Source code available at \url{https://github.com/philvasseur/Yahtzee-DQN-Thesis}}% 2.2 RL in Stochastic / Combinatorial Games

}% ----------------------------------------------------------------------------



@unpublished{yuan-2023-two-player-yahtzee,% tesauro1995tdgammon - already defined in Introduction

  author      = {Yuan, Max},% silver2016alphago - already defined in Introduction

  title       = {Using Deep Q-Learning to Play Two-Player Yahtzee},% silver2017alphagozero - already defined in Introduction

  note        = {Senior essay, Computer Science and Economics},% silver2018alphazero - already defined in Introduction

  institution = {Yale University},% moravcik2017deepstack - already defined in Introduction

  address     = {New Haven, CT, USA},% brown2018libratus - already defined in Introduction

  year        = {2023},% bard2020hanabi - already defined in Introduction

  month       = {12},% bertsekas1996tetris - already defined in Introduction

  advisor     = {James R. Glenn},% gabillon2013tetris - already defined in Introduction

  url         = {https://csec.yale.edu/senior-essays/fall-2023/using-deep-q-learning-play-two-player-yahtzee}% osband2016bootstrappeddqn - already defined in Introduction

}

% Early practical discussion of TD-learning issues (stability, function approximation, etc.).

% Similar efforts% Cite when talking about nuts-and-bolts TD, training details, and convergence concerns.

@misc{belaich-2024-yams,@article{tesauro1992practicaltd,

  author       = {Belaich, Alae},  title   = {Practical Issues in Temporal Difference Learning},

  title        = {{YAMS: Reinforcement Learning Project}},  author  = {Tesauro, Gerald},

  year         = {2024},  journal = {Machine Learning},

  howpublished = {\url{https://github.com/abelaich/YAMS-Reinforcement-Learning-Project}},  volume  = {8},

  note         = {GitHub repository, accessed 2025-11-16}  number  = {3-4},

}  pages   = {257--277},

  year    = {1992},

% ----------------------------------------------------------------------------  doi     = {10.1007/BF00992697}

% 2.2 RL in Stochastic / Combinatorial Games}

% ----------------------------------------------------------------------------

% NFSP: deep RL + fictitious play for imperfect-information (poker-style) games.

% NOTE: Most of these are already defined in Section 1:% Cite when you mention RL in imperfect-information, highly stochastic settings or mixtures of RL + supervised best-response learning.

% tesauro1995tdgammon, silver2016alphago, silver2017alphagozero, silver2018alphazero,@article{heinrich2016nfsp,

% moravcik2017deepstack, brown2018libratus, bard2020hanabi, bertsekas1996tetris,  title         = {Deep Reinforcement Learning from Self-Play in Imperfect-Information Games},

% gabillon2013tetris, osband2016bootstrappeddqn  author        = {Heinrich, Johannes and Silver, David},

  journal       = {CoRR},

% Early practical discussion of TD-learning issues (stability, function approximation, etc.).  volume        = {abs/1603.01121},

% Cite when talking about nuts-and-bolts TD, training details, and convergence concerns.  year          = {2016},

@article{tesauro1992practicaltd,  archiveprefix = {arXiv},

  title   = {Practical Issues in Temporal Difference Learning},  eprint        = {1603.01121}

  author  = {Tesauro, Gerald},}

  journal = {Machine Learning},

  volume  = {8},% ----------------------------------------------------------------------------

  number  = {3-4},% 2.3 RL Foundations / Algorithms

  pages   = {257--277},% ----------------------------------------------------------------------------

  year    = {1992},% These can be introduced in Related Work and then re-used later in Methods

  doi     = {10.1007/BF00992697}

}% Reinforcement Learning textbook

@book{sutton-2018-reinforcement-book,

% NFSP: deep RL + fictitious play for imperfect-information (poker-style) games.  title     = {Reinforcement Learning: An Introduction},

% Cite when you mention RL in imperfect-information, highly stochastic settings or mixtures of RL + supervised best-response learning.  author    = {Sutton, Richard S. and Barto, Andrew G.},

@article{heinrich2016nfsp,  year      = {2018},

  title         = {Deep Reinforcement Learning from Self-Play in Imperfect-Information Games},  edition   = {2nd},

  author        = {Heinrich, Johannes and Silver, David},  publisher = {MIT Press},

  journal       = {CoRR},  address   = {Cambridge, MA},

  volume        = {abs/1603.01121},  isbn      = {978-0262039246}

  year          = {2016},}

  archiveprefix = {arXiv},

  eprint        = {1603.01121}% REINFORCE, TD/TD-lambda, policy gradient, a3c, GAE, PPO

}@article{williams-1992-reinforce,

  author  = {Ronald J. Williams},

% ----------------------------------------------------------------------------  title   = {Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning},

% 2.3 RL Foundations / Algorithms  journal = {Machine Learning},

% ----------------------------------------------------------------------------  year    = {1992},

% These can be introduced in Related Work and then re-used later in Methods  volume  = {8},

  number  = {3-4},

% Reinforcement Learning textbook  pages   = {229--256},

@book{sutton-2018-reinforcement-book,  doi     = {10.1007/BF00992696}

  title     = {Reinforcement Learning: An Introduction},}

  author    = {Sutton, Richard S. and Barto, Andrew G.},

  year      = {2018},@article{sutton-1988-temporal-differences,

  edition   = {2nd},  author  = {Richard S. Sutton},

  publisher = {MIT Press},  title   = {Learning to Predict by the Methods of Temporal Differences},

  address   = {Cambridge, MA},  journal = {Machine Learning},

  isbn      = {978-0262039246}  year    = {1988},

}  volume  = {3},

  number  = {1},

% REINFORCE  pages   = {9--44},

@article{williams-1992-reinforce,  doi     = {10.1007/BF00115009}

  author  = {Ronald J. Williams},}

  title   = {Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning},

  journal = {Machine Learning},@inproceedings{sutton-2000-policy-gradient,

  year    = {1992},  author    = {Richard S. Sutton and David McAllester and Satinder Singh and Yishay Mansour},

  volume  = {8},  title     = {Policy Gradient Methods for Reinforcement Learning with Function Approximation},

  number  = {3-4},  booktitle = {Advances in Neural Information Processing Systems 12 (NIPS 1999)},

  pages   = {229--256},  year      = {2000},

  doi     = {10.1007/BF00992696}  pages     = {1057--1063}

}}



% Temporal Differences@inproceedings{mnih-2016-a3c,

@article{sutton-1988-temporal-differences,  author    = {Volodymyr Mnih and Adria Puigdom{\`e}nech Badia and Mehdi Mirza and Alex Graves and Timothy Lillicrap and Tim Harley and David Silver and Koray Kavukcuoglu},

  author  = {Richard S. Sutton},  title     = {Asynchronous Methods for Deep Reinforcement Learning},

  title   = {Learning to Predict by the Methods of Temporal Differences},  booktitle = {Proceedings of the 33rd International Conference on Machine Learning (ICML)},

  journal = {Machine Learning},  year      = {2016}

  year    = {1988},}

  volume  = {3},

  number  = {1},@article{schulman-2016-gae,

  pages   = {9--44},  author  = {John Schulman and Philipp Moritz and Sergey Levine and Michael I. Jordan and Pieter Abbeel},

  doi     = {10.1007/BF00115009}  title   = {High-Dimensional Continuous Control Using Generalized Advantage Estimation},

}  journal = {arXiv preprint arXiv:1506.02438},

  year    = {2016}

% Policy Gradient}

@inproceedings{sutton-2000-policy-gradient,

  author    = {Richard S. Sutton and David McAllester and Satinder Singh and Yishay Mansour},@article{schulman-2017-ppo,

  title     = {Policy Gradient Methods for Reinforcement Learning with Function Approximation},  author  = {John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},

  booktitle = {Advances in Neural Information Processing Systems 12 (NIPS 1999)},  title   = {Proximal Policy Optimization Algorithms},

  year      = {2000},  journal = {arXiv preprint arXiv:1707.06347},

  pages     = {1057--1063}  year    = {2017}

}}



% A3C@article{greensmith-2004-variance-reduction,

@inproceedings{mnih-2016-a3c,  author  = {Evan Greensmith and Peter L. Bartlett and Jonathan Baxter},

  author    = {Volodymyr Mnih and Adria Puigdom{\`e}nech Badia and Mehdi Mirza and Alex Graves and Timothy Lillicrap and Tim Harley and David Silver and Koray Kavukcuoglu},  title   = {Variance Reduction Techniques for Gradient Estimates in Reinforcement Learning},

  title     = {Asynchronous Methods for Deep Reinforcement Learning},  journal = {Journal of Machine Learning Research},

  booktitle = {Proceedings of the 33rd International Conference on Machine Learning (ICML)},  year    = {2004},

  year      = {2016}  volume  = {5},

}  pages   = {1471--1530}

}

% GAE (Generalized Advantage Estimation)

@article{schulman-2016-gae,% Variance / Stability in RL

  author  = {John Schulman and Philipp Moritz and Sergey Levine and Michael I. Jordan and Pieter Abbeel},@article{bjorck-2022-high-variance,

  title   = {High-Dimensional Continuous Control Using Generalized Advantage Estimation},  title         = {Is High Variance Unavoidable in RL? A Case Study in Continuous Control},

  journal = {arXiv preprint arXiv:1506.02438},  author        = {Bjorck, Johan and Gomes, Carla P. and Weinberger, Kilian Q.},

  year    = {2016}  journal       = {arXiv preprint arXiv:2110.11222},

}  year          = {2022},

  eprint        = {2110.11222},

% PPO  archiveprefix = {arXiv},

@article{schulman-2017-ppo,  primaryclass  = {cs.LG},

  author  = {John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},  url           = {https://arxiv.org/abs/2110.11222}

  title   = {Proximal Policy Optimization Algorithms},}

  journal = {arXiv preprint arXiv:1707.06347},

  year    = {2017}

}% ============================================================================

% 3. METHODS

% Variance reduction in policy gradients% ============================================================================

@article{greensmith-2004-variance-reduction,% This is where you get to be citation-dense without bloating the prose.

  author  = {Evan Greensmith and Peter L. Bartlett and Jonathan Baxter},

  title   = {Variance Reduction Techniques for Gradient Estimates in Reinforcement Learning},% ----------------------------------------------------------------------------

  journal = {Journal of Machine Learning Research},% 3.1 Environment / Game Definition

  year    = {2004},% ----------------------------------------------------------------------------

  volume  = {5},

  pages   = {1471--1530}% hasbro-2022-yahtzee-rules - already defined in Introduction

}% glenn-2006-optimal-yahtzee - already defined in Introduction

% pawlewicz-2011-multiplayer-yahtzee - already defined in Introduction

% High variance is unavoidable in RL

@article{bjorck-2022-high-variance,% ----------------------------------------------------------------------------

  title         = {Is High Variance Unavoidable in RL? A Case Study in Continuous Control},% 3.2 Architecture (state representation, activations, init, attention)

  author        = {Bjorck, Johan and Gomes, Carla P. and Weinberger, Kilian Q.},% ----------------------------------------------------------------------------

  journal       = {arXiv preprint arXiv:2110.11222},

  year          = {2022},% Capacity / width choices (already defined in Introduction)

  eprint        = {2110.11222},% horne-1994-bounds-rnn-fsm

  archiveprefix = {arXiv},% hanin-2017-bounded-width-relu

  primaryclass  = {cs.LG},

  url           = {https://arxiv.org/abs/2110.11222}% Normalization, activations, initialization

}@article{ba-2016-layernorm,

  author  = {Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton},

  title   = {Layer Normalization},

% ============================================================================  journal = {arXiv preprint arXiv:1607.06450},

% 3. METHODS  year    = {2016}

% ============================================================================}

% This is where you get to be citation-dense without bloating the prose.

@inproceedings{he-2015-delving-rectifiers,

% ----------------------------------------------------------------------------  author    = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},

% 3.1 Environment / Game Definition  title     = {Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification},

% ----------------------------------------------------------------------------  booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},

  year      = {2015},

% NOTE: The following are already defined in Section 1:  pages     = {1026--1034},

% hasbro-2022-yahtzee-rules  doi       = {10.1109/ICCV.2015.123}

% glenn-2006-optimal-yahtzee}

% pawlewicz-2011-multiplayer-yahtzee

@article{ramachandran-2017-swish,

% ----------------------------------------------------------------------------  author  = {Prajit Ramachandran and Barret Zoph and Quoc V. Le},

% 3.2 Architecture (state representation, activations, init, attention)  title   = {Searching for Activation Functions},

% ----------------------------------------------------------------------------  journal = {arXiv preprint arXiv:1710.05941},

  year    = {2017}

% Capacity / width choices (already defined in Section 1):}

% horne-1994-bounds-rnn-fsm

% hanin-2017-bounded-width-relu% GELU and SiLU activations

@article{elfwing-2018-silu,

% Layer Normalization  author  = {Elfwing, Stefan and Uchibe, Eiji and Doya, Kenji},

@article{ba-2016-layernorm,  title   = {Sigmoid-weighted linear units for neural network function approximation in reinforcement learning},

  author  = {Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton},  journal = {Neural Networks},

  title   = {Layer Normalization},  volume  = {107},

  journal = {arXiv preprint arXiv:1607.06450},  pages   = {3--11},

  year    = {2016}  year    = {2018},

}  doi     = {10.1016/j.neunet.2017.12.012}

}

% Kaiming/He initialization for ReLU networks

@inproceedings{he-2015-delving-rectifiers,@article{hendrycks-2016-gelu,

  author    = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},  author  = {Hendrycks, Dan and Gimpel, Kevin},

  title     = {Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification},  title   = {Gaussian Error Linear Units (GELUs)},

  booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},  journal = {arXiv preprint arXiv:1606.08415},

  year      = {2015},  year    = {2016},

  pages     = {1026--1034},  url     = {https://arxiv.org/abs/1606.08415}

  doi       = {10.1109/ICCV.2015.123}}

}

% Variance reduction and gradient clipping

% Swish activation function@article{greensmith-2004-variance-reduction,

@article{ramachandran-2017-swish,  author  = {Evan Greensmith and Peter L. Bartlett and Jonathan Baxter},

  author  = {Prajit Ramachandran and Barret Zoph and Quoc V. Le},  title   = {Variance Reduction Techniques for Gradient Estimates in Reinforcement Learning},

  title   = {Searching for Activation Functions},  journal = {Journal of Machine Learning Research},

  journal = {arXiv preprint arXiv:1710.05941},  year    = {2004},

  year    = {2017}  volume  = {5},

}  pages   = {1471--1530}

}

% SiLU (Sigmoid-Weighted Linear Units) activation for RL

@article{elfwing-2018-silu-rl,@article{pascanu-2013-rnn-clipping,

  author  = {Stefan Elfwing and Eiji Uchibe and Kenji Doya},  author  = {Razvan Pascanu and Tomas Mikolov and Yoshua Bengio},

  title   = {Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning},  title   = {On the Difficulty of Training Recurrent Neural Networks},

  journal = {Neural Networks},  journal = {arXiv preprint arXiv:1211.5063},

  year    = {2018},  year    = {2013}

  volume  = {107},}

  pages   = {3--11},

  doi     = {10.1016/j.neunet.2017.12.012}% Normalization, activations, initialization

}@article{ba-2016-layernorm,

  author  = {Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton},

% GELU activation  title   = {Layer Normalization},

@article{hendrycks-2016-gelu,  journal = {arXiv preprint arXiv:1607.06450},

  author  = {Hendrycks, Dan and Gimpel, Kevin},  year    = {2016}

  title   = {Gaussian Error Linear Units (GELUs)},}

  journal = {arXiv preprint arXiv:1606.08415},

  year    = {2016},@article{ramachandran-2017-swish,

  url     = {https://arxiv.org/abs/1606.08415}  author  = {Prajit Ramachandran and Barret Zoph and Quoc V. Le},

}  title   = {Searching for Activation Functions},

  journal = {arXiv preprint arXiv:1710.05941},

% RAdam optimizer (variance of adaptive learning rate)  year    = {2017}

@article{liu-2019-radam,}

  title         = {On the Variance of the Adaptive Learning Rate and Beyond},

  author        = {Liu, Liyuan and Jiang, Haoming and He, Pengcheng and Chen, Weizhu and Liu, Xiaodong and Gao, Jianfeng and Han, Jiawei},@article{elfwing-2018-silu-rl,

  journal       = {arXiv preprint arXiv:1908.03265},  author  = {Stefan Elfwing and Eiji Uchibe and Kenji Doya},

  year          = {2019},  title   = {Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning},

  eprint        = {1908.03265},  journal = {Neural Networks},

  archiveprefix = {arXiv},  year    = {2018},

  primaryclass  = {cs.LG},  volume  = {107},

  url           = {https://arxiv.org/abs/1908.03265}  pages   = {3--11},

}  doi     = {10.1016/j.neunet.2017.12.012}

}

% Attention mechanism

@inproceedings{vaswani-2017-attention,@inproceedings{he-2015-delving-rectifiers,

  title     = {Attention {I}s {A}ll {Y}ou {N}eed},  author    = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},

  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob  title     = {Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification},

               and Jones, Llion and Gomez, Aidan N. and Kaiser, {\L}ukasz and Polosukhin, Illia},  booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},

  booktitle = {Advances in Neural Information Processing Systems},  year      = {2015},

  editor    = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R.  pages     = {1026--1034},

               and Vishwanathan, S. and Garnett, R.},  doi       = {10.1109/ICCV.2015.123}

  volume    = {30},}

  year      = {2017}

}

% Multiplayer Yahtzee

% ----------------------------------------------------------------------------@inproceedings{pawlewicz-2011-multiplayer-yahtzee,

% 3.3 RL Algorithms & Optimization (REINFORCE / A2C / PPO / tricks)  author    = {Pawlewicz, Jakub},

% ----------------------------------------------------------------------------  editor    = {van den Herik, H. Jaap

               and Iida, Hiroyuki

% Core algorithm references (already defined in Section 2):               and Plaat, Aske},

% sutton-2018-reinforcement-book  title     = {Nearly Optimal Computer Play in Multi-player Yahtzee},

% williams-1992-reinforce  booktitle = {Computers and Games},

% sutton-1988-temporal-differences  year      = {2011},

% sutton-2000-policy-gradient  publisher = {Springer Berlin Heidelberg},

% mnih-2016-a3c  address   = {Berlin, Heidelberg},

% schulman-2016-gae  pages     = {250--262},

% schulman-2017-ppo  abstract  = {Yahtzee is the most popular commercial dice game in the world. It can be played either by one or many players. In case of the single-player version, optimal computer strategies both for maximizing the expected average score and for maximizing the probability of beating a particular score are already known. However, when it comes to the multi-player version, those approaches are far too resource intensive and thus are not able to develop an optimal strategy given the current hardware.},

% greensmith-2004-variance-reduction  isbn      = {978-3-642-17928-0}

}

% Gradient clipping for RNN stability

@article{pascanu-2013-rnn-clipping,% =========================================================

  author  = {Razvan Pascanu and Tomas Mikolov and Yoshua Bengio},% Warmup / Effective Learning Rate Theory (General / DL)

  title   = {On the Difficulty of Training Recurrent Neural Networks},% =========================================================

  journal = {arXiv preprint arXiv:1211.5063},

  year    = {2013}@article{kalra-2024-warmup,

}  title         = {Why Warmup the Learning Rate? Underlying Mechanisms and Improvements},

  author        = {Kalra, Dayal Singh and Barkeshli, Maissam},

% bjorck-2022-high-variance - already defined in Section 2  journal       = {arXiv preprint arXiv:2406.09405},

  year          = {2024},

% Normalization and effective learning rates in RL  eprint        = {2406.09405},

@article{lyle-2024-normalization-rl,  archiveprefix = {arXiv},

  title         = {Normalization and effective learning rates in reinforcement learning},  primaryclass  = {cs.LG},

  author        = {Lyle, Clare and Zheng, Zeyu and Khetarpal, Khimya and Martens, James and van Hasselt, Hado and Pascanu, Razvan and Dabney, Will},  url           = {https://arxiv.org/abs/2406.09405}

  journal       = {arXiv preprint arXiv:2407.01800},}

  year          = {2024},

  eprint        = {2407.01800},@article{liu-2025-warmup-theory,

  archiveprefix = {arXiv},  title         = {Theoretical Analysis on how Learning Rate Warmup Accelerates Convergence},

  primaryclass  = {cs.LG},  author        = {Liu, Yuxing and Ge, Yuze and Pan, Rui and Kang, An and Zhang, Tong},

  url           = {https://arxiv.org/abs/2407.01800}  journal       = {arXiv preprint arXiv:2509.07972},

}  year          = {2025},

  eprint        = {2509.07972},

% Learning rate warmup mechanisms  archiveprefix = {arXiv},

@article{kalra-2024-warmup,  primaryclass  = {cs.LG},

  title         = {Why Warmup the Learning Rate? Underlying Mechanisms and Improvements},  url           = {https://arxiv.org/abs/2509.07972}

  author        = {Kalra, Dayal Singh and Barkeshli, Maissam},}

  journal       = {arXiv preprint arXiv:2406.09405},

  year          = {2024},@article{lyle-2024-normalization-rl,

  eprint        = {2406.09405},  title         = {Normalization and effective learning rates in reinforcement learning},

  archiveprefix = {arXiv},  author        = {Lyle, Clare and Zheng, Zeyu and Khetarpal, Khimya and Martens, James and van Hasselt, Hado and Pascanu, Razvan and Dabney, Will},

  primaryclass  = {cs.LG},  journal       = {arXiv preprint arXiv:2407.01800},

  url           = {https://arxiv.org/abs/2406.09405}  year          = {2024},

}  eprint        = {2407.01800},

  archiveprefix = {arXiv},

% Theoretical analysis of learning rate warmup  primaryclass  = {cs.LG},

@article{liu-2025-warmup-theory,  url           = {https://arxiv.org/abs/2407.01800}

  title         = {Theoretical Analysis on how Learning Rate Warmup Accelerates Convergence},}

  author        = {Liu, Yuxing and Ge, Yuze and Pan, Rui and Kang, An and Zhang, Tong},

  journal       = {arXiv preprint arXiv:2509.07972},@article{liu-2019-radam,

  year          = {2025},  title         = {On the Variance of the Adaptive Learning Rate and Beyond},

  eprint        = {2509.07972},  author        = {Liu, Liyuan and Jiang, Haoming and He, Pengcheng and Chen, Weizhu and Liu, Xiaodong and Gao, Jianfeng and Han, Jiawei},

  archiveprefix = {arXiv},  journal       = {arXiv preprint arXiv:1908.03265},

  primaryclass  = {cs.LG},  year          = {2019},

  url           = {https://arxiv.org/abs/2509.07972}  eprint        = {1908.03265},

}  archiveprefix = {arXiv},

  primaryclass  = {cs.LG},

  url           = {https://arxiv.org/abs/1908.03265}

% ============================================================================}

% 4. SINGLE-TURN RESULTS & ABLATIONSGive 

% ============================================================================% Variance / Stability in RL

% This section usually doesn't need new citations, but you can refer back to:

%@article{bjorck-2022-high-variance,

% Algorithm refs when interpreting why REINFORCE/variance gives noisy behavior:  title         = {Is High Variance Unavoidable in RL? A Case Study in Continuous Control},

%   - williams-1992-reinforce (Section 2)  author        = {Bjorck, Johan and Gomes, Carla P. and Weinberger, Kilian Q.},

%   - greensmith-2004-variance-reduction (Section 2)  journal       = {arXiv preprint arXiv:2110.11222},

%   - bjorck-2022-high-variance (Section 2)  year          = {2022},

%  eprint        = {2110.11222},

% Architecture refs if an ablation is "turning off LayerNorm or residuals:"  archiveprefix = {arXiv},

%   - ba-2016-layernorm (Section 3)  primaryclass  = {cs.LG},

%   - he-2015-delving-rectifiers (Section 3)  url           = {https://arxiv.org/abs/2110.11222}

%   - ramachandran-2017-swish (Section 3)}

%   - elfwing-2018-silu-rl (Section 3)





% ============================================================================

% 5. FULL-GAME EXPERIMENTS (Transfer vs. Scratch, Algo Comparisons)% DP Solvers for Yahtzee

% ============================================================================@techreport{glenn-2006-optimal-yahtzee,

% Again, no big new references, but several re-uses:  author      = {Jeffrey R. Glenn},

%  title       = {An Optimal Strategy for Yahtzee},

% When talking about transfer vs learning from scratch in the context of games:  institution = {Loyola College in Maryland, Department of Computer Science},

%   - silver2017alphagozero (Section 1)  number      = {CS-TR-0002},

%   - silver2018alphazero (Section 1)  year        = {2006},

%  url         = {https://gunpowder.cs.loyola.edu/~jglenn/research/optimal_yahtzee.pdf}

% When justifying PPO vs REINFORCE / A2C:}

%   - schulman-2017-ppo (Section 2)

%   - mnih-2016-a3c (Section 2)@inproceedings{glenn-2007-solitaire-yahtzee,

%   - sutton-2000-policy-gradient (Section 2)  author    = {Jeffrey R. Glenn},

%  title     = {Computer Strategies for Solitaire Yahtzee},

% When discussing credit assignment and instability in full-game scores:  booktitle = {2007 IEEE Symposium on Computational Intelligence and Games (CIG)},

%   - tesauro1992practicaltd (Section 2)  year      = {2007},

%   - bjorck-2022-high-variance (Section 2)  pages     = {132--139},

%   - lyle-2024-normalization-rl (Section 3)  doi       = {10.1109/CIG.2007.368085}

%}

% If you explicitly mention exploration issues in the full game:

%   - osband-2016-bootstrappeddqn (Section 1)@misc{verhoeff-1999-solitaire-yahtzee,

  author       = {Tom Verhoeff},

  title        = {Optimal Solitaire Yahtzee Strategies (slides)},

% ============================================================================  year         = {1999},

% 6. DISCUSSION  howpublished = {\url{https://www-set.win.tue.nl/~wstomv/misc/yahtzee/slides-2up.pdf}}

% ============================================================================}

% This is where you connect your observations back to the literature.

%

% High variance and stability:% Student projects

%   - greensmith-2004-variance-reduction (Section 2)@unpublished{kang-2018-yahtzee-rl,

%   - bjorck-2022-high-variance (Section 2)  author = {Kang, Minhyung and Schroeder, Luca},

%   - pascanu-2013-rnn-clipping (Section 3)  title  = {Reinforcement Learning for Solving Yahtzee},

%   - lyle-2024-normalization-rl (Section 3)  year   = {2018},

%  note   = {AA228: Decision Making under Uncertainty, Stanford University, class project report},

% "Our results mirror known difficulties in long-horizon RL for games":  url    = {https://web.stanford.edu/class/aa228/reports/2018/final75.pdf}

%   - tesauro1995tdgammon (Section 1)}

%   - bertsekas1996tetris (Section 1)

%   - gabillon2013tetris (Section 1)@unpublished{yuan-2023-two-player-yahtzee,

%  author      = {Yuan, Max},

% "We're in the same general lineage as AlphaGo-style self-play":  title       = {Using Deep Q-Learning to Play Two-Player Yahtzee},

%   - silver2016alphago (Section 1)  note        = {Senior essay, Computer Science and Economics},

%   - silver2017alphagozero (Section 1)  institution = {Yale University},

%   - silver2018alphazero (Section 1)  address     = {New Haven, CT, USA},

%   - heinrich2016nfsp (Section 2)  year        = {2023},

%   - moravcik2017deepstack (Section 1)  month       = {12},

%   - brown2018libratus (Section 1)  advisor     = {James R. Glenn},

%   - bard2020hanabi (Section 1)  url         = {https://csec.yale.edu/senior-essays/fall-2023/using-deep-q-learning-play-two-player-yahtzee}

%}

% Future work / architecture:

%   - vaswani-2017-attention (Section 3)@bachelorsthesis{vasseur-2019-strategy-ladders,

  author  = {Vasseur, Philip},

  title   = {Using Deep Q-Learning to Compare Strategy Ladders of Yahtzee},

% ============================================================================  school  = {Yale University},

% 7. APPENDIX  type    = {Undergraduate thesis, Department of Computer Science},

% ============================================================================  address = {New Haven, CT},

% Perfect place to drop any references that are too niche for the main text.  year    = {2019},

%  url     = {https://raw.githubusercontent.com/philvasseur/Yahtzee-DQN-Thesis/dcf2bfe15c3b8c0ff3256f02dd3c0aabdbcbc9bb/webpage/final_report.pdf},

% Deep theoretical LR warmup papers:  note    = {Source code available at \url{https://github.com/philvasseur/Yahtzee-DQN-Thesis}}

%   - kalra-2024-warmup (Section 3)}

%   - liu-2025-warmup-theory (Section 3)

%

% Capacity/width bounds:

%   - horne-1994-bounds-rnn-fsm (Section 1)% Similar efforts

%   - hanin-2017-bounded-width-relu (Section 1)@misc{belaich-2024-yams,

%  author       = {Belaich, Alae},

% Extra methodology/background:  title        = {{YAMS: Reinforcement Learning Project}},

%   - liu-2019-radam (Section 3)  year         = {2024},

  howpublished = {\url{https://github.com/abelaich/YAMS-Reinforcement-Learning-Project}},
  note         = {GitHub repository, accessed 2025-11-16}
}


% Attention
@inproceedings{vaswani-2017-attention,
  title     = {Attention {I}s {A}ll {Y}ou {N}eed},
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob
               and Jones, Llion and Gomez, Aidan N. and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R.
               and Vishwanathan, S. and Garnett, R.},
  volume    = {30},
  year      = {2017}
}

% Reinforcement Learning textbook
@book{sutton-2018-reinforcement-book,
  title     = {Reinforcement Learning: An Introduction},
  author    = {Sutton, Richard S. and Barto, Andrew G.},
  year      = {2018},
  edition   = {2nd},
  publisher = {MIT Press},
  address   = {Cambridge, MA},
  isbn      = {978-0262039246}
}

@manual{hasbro-2022-yahtzee-rules,
  title        = {YAHTZEE Game: Instructions},
  author       = {{Hasbro, Inc.}},
  year         = {2022},
  note         = {Official rules and instructions},
  howpublished = {\url{https://instructions.hasbro.com/en-nz/instruction/yahtzee-game}}
}


% Classic case study of TD learning beating humans in a highly stochastic board game (backgammon).
% Cite when motivating self-play + value function approximation in stochastic environments.
@article{tesauro1995tdgammon,
  title   = {Temporal Difference Learning and TD-Gammon},
  author  = {Tesauro, Gerald},
  journal = {Communications of the ACM},
  volume  = {38},
  number  = {3},
  pages   = {58--68},
  year    = {1995},
  doi     = {10.1145/203330.203343}
}

% Early practical discussion of TD-learning issues (stability, function approximation, etc.).
% Cite when talking about nuts-and-bolts TD, training details, and convergence concerns.
@article{tesauro1992practicaltd,
  title   = {Practical Issues in Temporal Difference Learning},
  author  = {Tesauro, Gerald},
  journal = {Machine Learning},
  volume  = {8},
  number  = {3-4},
  pages   = {257--277},
  year    = {1992},
  doi     = {10.1007/BF00992697}
}

% Original AlphaGo paper: deep nets + MCTS, human data + self-play.
% Cite when introducing deep RL + tree search for board games or superhuman Go with human data.
@article{silver2016alphago,
  title   = {Mastering the Game of Go with Deep Neural Networks and Tree Search},
  author  = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal = {Nature},
  volume  = {529},
  number  = {7587},
  pages   = {484--489},
  year    = {2016},
  doi     = {10.1038/nature16961}
}

% AlphaGo Zero: no human data, pure self-play, massive compute.
% Cite when emphasizing learning from scratch and strong self-play training regimes.
@article{silver2017alphagozero,
  title   = {Mastering the Game of Go without Human Knowledge},
  author  = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and others},
  journal = {Nature},
  volume  = {550},
  number  = {7676},
  pages   = {354--359},
  year    = {2017},
  doi     = {10.1038/nature24270}
}

% AlphaZero: single general algorithm for Go/Chess/Shogi via self-play.
% Cite when framing your method as AlphaZero-style self-play for game X or arguing for generality.
@article{silver2018alphazero,
  title   = {A General Reinforcement Learning Algorithm that Masters Chess, Shogi, and Go through Self-Play},
  author  = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
  journal = {Science},
  volume  = {362},
  number  = {6419},
  pages   = {1140--1144},
  year    = {2018},
  doi     = {10.1126/science.aar6404}
}

% NFSP: deep RL + fictitious play for imperfect-information (poker-style) games.
% Cite when you mention RL in imperfect-information, highly stochastic settings or mixtures of RL + supervised best-response learning.
@article{heinrich2016nfsp,
  title         = {Deep Reinforcement Learning from Self-Play in Imperfect-Information Games},
  author        = {Heinrich, Johannes and Silver, David},
  journal       = {CoRR},
  volume        = {abs/1603.01121},
  year          = {2016},
  archiveprefix = {arXiv},
  eprint        = {1603.01121}
}

% Hanabi challenge paper.
% Cite when motivating cooperative multi-agent RL, partial observability, or hard stochastic card games.
@article{bard2020hanabi,
  title   = {The Hanabi Challenge: A New Frontier for {AI} Research},
  author  = {Bard, Nolan and Foerster, Jakob N. and Chandar, Sarath and Burch, Neil and Lanctot, Marc and Song, H. Francis and Parisotto, Emilio and Dumoulin, Vincent and Moitra, Subhodeep and Hughes, Edward and Dunning, Iain and Mourad, Shibl and Larochelle, Hugo and Bellemare, Marc G. and Bowling, Michael},
  journal = {Artificial Intelligence},
  volume  = {280},
  pages   = {103216},
  year    = {2020},
  doi     = {10.1016/j.artint.2019.103216}
}

% DeepStack poker AI.
% Cite when discussing RL + search + value networks in high-variance, imperfect-information games (No-Limit Texas Holdem).
@article{moravcik2017deepstack,
  title   = {DeepStack: Expert-Level Artificial Intelligence in Heads-Up No-Limit Poker},
  author  = {Morav{\v{c}}{\'\i}k, Matej and Schmid, Martin and Burch, Neil and Lis{\'y}, Viliam and Morrill, Dustin and Bard, Nolan and Davis, Trevor and Waugh, Kevin and Johanson, Michael and Bowling, Michael},
  journal = {Science},
  volume  = {356},
  number  = {6337},
  pages   = {508--513},
  year    = {2017},
  doi     = {10.1126/science.aam6960}
}

% Libratus poker AI.
% Cite when referencing game-theoretic RL / equilibrium-finding in complex stochastic games vs. human pros.
@article{brown2018libratus,
  title   = {Superhuman {AI} for Heads-Up No-Limit Poker: {Libratus} Beats Top Professionals},
  author  = {Brown, Noam and Sandholm, Tuomas},
  journal = {Science},
  volume  = {359},
  number  = {6374},
  pages   = {418--424},
  year    = {2018},
  doi     = {10.1126/science.aao1733}
}

% Early ADP/TD work on Tetris.
% Cite when talking about approximate dynamic programming on huge, stochastic MDPs (e.g., Tetris as a benchmark for instability/difficulty).
@techreport{bertsekas1996tetris,
  title       = {Temporal Differences-Based Policy Iteration and Applications in Neuro-Dynamic Programming},
  author      = {Bertsekas, Dimitri P. and Ioffe, Sergey},
  institution = {Laboratory for Information and Decision Systems, MIT},
  number      = {LIDS-P-2349},
  year        = {1996}
}

% Tetris is actually hard but ADP can work paper.
% Cite when you want a modern-ish RL/ADP reference on stochastic combinatorial games beyond Go/Atari (e.g., as evidence RL can crack tough puzzle-like domains).
@inproceedings{gabillon2013tetris,
  title     = {Approximate Dynamic Programming Finally Performs Well in the Game of Tetris},
  author    = {Gabillon, Victor and Ghavamzadeh, Mohammad and Lazaric, Alessandro and Scherrer, Bruno},
  booktitle = {Advances in Neural Information Processing Systems},
  volume    = {26},
  year      = {2013}
}

% Bootstrapped DQN for deep exploration.
% Cite when you need a canonical exploration in deep RL reference or when arguing about handling high stochasticity via better exploration.
@inproceedings{osband2016bootstrappeddqn,
  title     = {Deep Exploration via Bootstrapped {DQN}},
  author    = {Osband, Ian and Blundell, Charles and Pritzel, Alexander and Van Roy, Benjamin},
  booktitle = {Advances in Neural Information Processing Systems},
  volume    = {29},
  year      = {2016}
}
