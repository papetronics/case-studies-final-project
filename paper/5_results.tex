\section{Results}

\subsection{Single-Turn Results}
% <600-700 words>
\subsubsection{Baseline Single-Turn Performance}
For state representation, the baseline model utilizes:

$$\phi(\mathbf{s}) = \big[\phi_{\mathrm{dice}}^{\mathrm{combined}}(\mathbf{d}), \phi_{\mathrm{cat}}(\mathbf{c}), \phi_{\mathrm{bonus}}(\mathbf{c}), \phi_{\mathrm{rolls}}(r)\big]$$

For outputs, it uses Bernoulli rolling actions and categorical scoring actions.
The single turn model has a short horizon (3 steps); REINFORCE was the natural choice here.
We trained on <X> single-turn episodes, using a batch size of <Y> episodes, for <Z> total gradient updates.

Although it does not nearly reach optimal performance, it performs surprisingly well over the full game; this is likely due to the high correlation between single-turn and full-game optimal actions.
However, we suspected target leakage (selecting parameters and architectures based on full-game performance) could also play a role and analyze the full-game vs. single-turn tradeoff in Section~\ref{sec:tradeoff-curve}.

\input{figures/single_turn_performance}

\input{figures/single_turn_fullgame}

\subsubsection{Single vs Full-game Tradeoff Curve}
\label{sec:tradeoff-curve}
To understand the tradeoff between single-turn and full-game performance, we ablated our model using small changes to various hyperparameters and captured
the resulting performance on both the primary single-turn score, as well as the auxiliary full-game score.
We expect that there is a Pareto frontier between these two objectives, and that some hyperparameter choices push performance towards one or the other.

As expected, we can see that full game performance generally increases linearly with single-turn performance.
However, at very high levels of full-game performance, single-turn performance begins to plateau, and even decline slightly.
Since the single-turn model does not have access to the full game context, these are actually imperfectly optimizing their purported objective.
This indicates that selecting hyperparameters for a single-turn model based on full-game performance could indeed be a form of target leakage.

\input{figures/pareto_frontier}

\subsection{Full-Game Results}
% <600-700 words>
For the full-game model, we added several additional features to the state representation: $\phi_{\mathrm{progress}}(t)$ and $\phi_{\mathrm{potential}}(\mathbf{d}, \mathbf{c})$
while reusing the same underlying neural network architecture as the single-turn model. These additions were necessary to provide the model with sufficient context to make long-term strategic decisions.
While the $\phi_{\mathrm{progress}}(t)$ feature could be inferred from the scorecard, the model struggled to do so reliably.
We intentionally omitted the $\phi_{\mathrm{potential}}(\mathbf{d}, \mathbf{c})$ feature in single turn, as we wanted to ensure the model was capable of learning to reason about category potential on its own,
but found it to be necessary, especially with REINFORCE.

\subsubsection{Algorithm Comparison: REINFORCE, A2C, PPO}

REINFORCE proved challenging to optimize to high performance levels given our fixed training budget of 1 million full-game episodes (39 million steps).
It was highly sensitive to hyperparameters such as the critic coefficient, the entropy bonus, and batch size.
We also found that REINFORCE simply required more training data to converge at a reliable performance level across seeds; our implementation was trained on 1,000,000 games.
However, after optimization we were able to achieve reasonable performance, scoring a mean of <X> points on average over 10,000 full games.

Our most successful algorithm was TD(0)-style Actor-Critic (A2C). We found is easiest to tune and with an immediate performance boost over REINFORCE.
This was the algorithm we use for the ablation studies. With a fixed training budget of 1 million full-games,
A2C was able to approach DP-optimal performance.

We also attempted to use Proximal Policy Optimization (PPO) with TD(0), but found it difficult to tune effectively within our computational budget. Each PPO rollout requires
$k$ epochs of minibatch updates, which significantly increases training time compared to A2C and REINFORCE. Sample efficiency wasn't a huge factor for us, since Yahtzee is so easy to simulate.
For fair comparison to the other algorithms, we had to reduce the total number of games seen during training by a factor of $k$.
However, it is possible PPO could reach or surpass A2C performance with more extensive hyperparameter tuning.

\input{figures/baselines_training_graph}

\input{figures/baselines_bar_chart}

\input{figures/baselines_category_bar_chart}

\subsubsection{Representational Ablations}

While a number of additional representational choices were explored, one of the most important is the state representation of the dice.
One important thing to note is that our environment always sorts the dice in ascending order before passing them to the agent.
If this was not done, the network would be forced to waste capacity on understanding permutations of the same dice configuration,
which we found early on was a significant impediment to learning.

We wanted to highlight the importance of using a combined representation of both one-hot encodings and counts-based encodings of the dice.
While the network could theoretically learn to reconstruct either representation from the other, in practice we found that using both improved performance.

\input{figures/dice_representation_bonus}

For the full-game model, we added several additional features beyond the single-turn representation.
To understand the importance of each, they were ablated individually.

\input{figures/feature_ablation_bonus}

Lastly, we tested our hypothesis that a 32-way categorical would provide beneficial to complex actions that required specific combinations of dice to be held.
\input{figures/action_representation}

\subsubsection{Architectural Ablations}

We performed a simple grid search ablation to understand if our chosen architecture of 3 hidden layers of 600 units each was optimal.
Yahtzee is a fairly complex game, so we expected shorter, but wider networks to perform best. Note that each of these has a different number of total parameters,
so this is not a pure ablation of depth vs. width.

\input{figures/architecture_heatmap}

Based on \cite{bjorck-2022-high-variance}, we hypothesized that layer normalization \cite{ba-2016-layernorm} would improve training stability and performance
and used it in all of our main experiments. This was ablated to understand its true impact.

\input{figures/layernorm}

Lastly, we used Swish activation functions in our network for all of our main experiments, as it had been reported to work well in RL settings \cite{elfwing2017sigmoidweightedlinearunitsneural}.
We ablated this to ReLU to understand its true impact.

\input{figures/activations}

\subsubsection{Credit Assignment: TD(0) vs GAE}

Later in this research, we noticed the main issue with our network was that it was struggling to earn the bonus, learning it very slowly.
We first hypothesized that this was due to high variance in REINFORCE, so we switched to A2C with TD(0) targets. However, the issue
persisted. We then hypothesized that the TD(0) targets were not providing sufficient credit assignment for the long-term bonus reward,
so we switched to GAE with various $\lambda$ values to understand if this would help.

Unfortunately, we found that GAE did not improve performance over TD(0), and values that were too high ($\lambda \geq 0.8$)
significantly degraded performance.

\input{figures/gae_mean_score}
\input{figures/gae_bonus}

\subsubsection{Entropy Sensitivity}
During experimentation, we noticed that the entropy regularization coefficients had a significant impact on training stability and final performance.
With no regularization, the policy quickly converges to a deterministic policy that fails to explore sufficiently.
With too much regularization, the policy would not perform well enough to quickly learn strategies for earning Yahtzee's and the bonus.
To better understand this sensitivity, we trained models under three different entropy regimes: Low Entropy, Baseline, and High Entropy.

\begin{table}[H]
    \centering
    \small
    \caption{Entropy regime definitions}
    \label{tab:entropy-regime-definitions}
    \begin{tabular}{lcccc}
        \hline
        \textbf{Regime} & $\beta_{\mathrm{roll}}$ & $\beta_{\mathrm{score}}$ & Hold / Anneal \\
        \hline
        None            & 0.0                     & 0.0                      & 0 / 0         \\
        Low             & 0.05 $\rightarrow$ 0.01 & 0.01 $\rightarrow$ 0.005 & 0.2 / 0.4     \\
        Baseline        & 0.1 $\rightarrow$ 0.02  & 0.03 $\rightarrow$ 0.01  & 0.3 / 0.6     \\
        High            & 0.2 $\rightarrow$ 0.04  & 0.06 $\rightarrow$ 0.02  & 0.35 / 0.65   \\
        \hline
    \end{tabular}
\end{table}

\input{figures/entropy_mean_score}

\input{figures/entropy_rolling_entropy}

\begin{table}[H]
    \centering
    \small
    \caption{Entropy regime performance (placeholder data)}
    \label{tab:entropy-regimes}
    \begin{tabular}{lccc}
        \hline
        \textbf{Regime} & \textbf{Mean Score} & \textbf{Bonus \%} & \textbf{Yahtzee \%} \\
        \hline
        Low Entropy     & $<X> \pm <Y>$       & <X>               & <Y>                 \\
        Baseline        & $<X> \pm <Y>$       & <X>               & <Y>                 \\
        High Entropy    & $<X> \pm <Y>$       & <X>               & <Y>                 \\
        \hline
    \end{tabular}
\end{table}

\subsubsection{Summary}
In summary, we found that A2C with TD(0) targets, a combined dice representation, Layer Normalization, SILU activations,
and carefully tuned entropy regularization produced the best results.

\begin{table*}[t]
    \centering
    \small
    \caption{Full-game performance summary (placeholder data)}
    \label{tab:full-game-summary}
    \begin{tabular}{lcccccc}
        \hline
        \textbf{Algorithm}         & \textbf{Training Budget} & \textbf{Mean Score} & \textbf{Std Dev} & \textbf{Bonus Rate (\%)} & \textbf{Yahtzee Rate (\%)} & \textbf{$\geq$250 (\%)} \\
        \hline
        DP Optimal                 & --                       & $<X>$               & --               & $<X>$                    & $<Y>$                      & $<Z>$                   \\
        A2C                        & 250K games               & $<X>$               & $<Y>$            & $<X>$                    & $<Y>$                      & $<Z>$                   \\
        A2C                        & 1M games                 & $<X>$               & $<Y>$            & $<X>$                    & $<Y>$                      & $<Z>$                   \\
        A2C (lower entropy floor)  & 5M games                 & $<X>$               & $<Y>$            & $<X>$                    & $<Y>$                      & $<Z>$                   \\
        PPO ($\lambda=0.3$, $k=3$) & 250K games               & $<X>$               & $<Y>$            & $<X>$                    & $<Y>$                      & $<Z>$                   \\
        REINFORCE (full-game)      & 250K games               & $<X>$               & $<Y>$            & $<X>$                    & $<Y>$                      & $<Z>$                   \\
        REINFORCE (full-game)      & 1M games                 & $<X>$               & $<Y>$            & $<X>$                    & $<Y>$                      & $<Z>$                   \\
        REINFORCE (single-turn)    & 500K games               & $<X>$               & $<Y>$            & $<X>$                    & $<Y>$                      & $<Z>$                   \\
        \hline
    \end{tabular}
\end{table*}

For our best configuration, A2C trained over 5,000,000 games, the final score distribution is compared to DP-optimal in Table \ref{tab:dp-score-distribution}.
\begin{table}[H]
    \centering
    \small
    \caption{Score distribution (placeholder data)}
    \label{tab:dp-score-distribution}
    \begin{tabular}{cc}
        \hline
        \textbf{$n$} & \textbf{$P(\text{score} \geq n)$} \\
        \hline
        50           & $<X>$                             \\
        100          & $<X>$                             \\
        150          & $<X>$                             \\
        200          & $<X>$                             \\
        250          & $<X>$                             \\
        300          & $<X>$                             \\
        400          & $<X>$                             \\
        500          & $<X>$                             \\
        750          & $<X>$                             \\
        1000         & $<X>$                             \\
        1250         & $<X>$                             \\
        1500         & $<X>$                             \\
        \hline
    \end{tabular}
\end{table}

\subsection{Policy Analysis}
% <300-500 words>
\subsubsection{Category Usage}
To understand the overall performance of the agents, we compare their average scores in each category as a percentage of the DP-optimal score for that category
in figures \ref{fig:category-upper} and \ref{fig:category-lower}.

The primary differentiators between our RL agents and the DP optimal solution are in the upper section (and bonus), four-of-a-kind and Yahtzee.
Performance in the upper section is critical to achieving a high overall score, as it enables the bonus however it appears to be traded off against
the four-of-a-kind; it seems the agents prefer to take the immediate points from the 5th die rather than place themselves in a safer position to earn the bonus later.

Yahtzee's high performance was interesting; it requires agents to be performing at a competent level across a single turn in order to earn. We contrast the learning
curve of the Yahtzee category with that of the upper bonus in Figure <INSERT A FIGURE FOR THIS>. The interesting takeaway here is that agents appear to
exhibit a "mode shift" in their strategy once they figure out Yahtzee, whereas the bonus is learned more gradually over time.

\input{figures/category_upper}

\input{figures/category_lower}

\subsubsection{Strategy Comparison Across Agents}
We also compared some high-level strategy metrics across our different agents to understand how their learned policies differed.
First, we analyzed the distribution of the first category choice made by each agent in Figure \ref{fig:strategy-first-category}.
Next, we analyzed each category to understand at what point in the game the agent typically filled it, shown in Figure \ref{fig:strategy-fill-turn}.

\input{figures/strategy_first_category}

\input{figures/strategy_fill_turn}
