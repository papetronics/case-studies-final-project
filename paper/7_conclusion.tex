
\section{Conclusion and Future Work}
% <200-300 words>

Learning a robust policy for \textit{Yahtzee} using reinforcement learning presents several interesting challenges and insights.
We showed that with appropriate algorithmic choices, it is possible to approach near-optimal performance using self-play alone.
Our results back up theoretical results in the literature regarding training stability and sample efficiency of common RL algorithms.
Our analysis of learned policies showed that these algorithms often struggle to learn rare, yet high-reward strategies, especially if they require strong coherence over longer time horizons.

Future research could be done to find architectures, samples, and learning methods that allow the model to better approximate optimal play, more efficiently.
Transfer learning could be explored further to see if knowledge from single-turn optimization could be effectively transferred to full-game, multiplayer Yahtzee, or other variants of the game.
For example, curriculum learning approaches, where the agent is gradually exposed to more complex scenarios over time, could be used to help the model overcome some challenges outlined in this paper.
For the multiplayer setting, future work could explore permutation-invariant architectures such as Deep Sets \cite{zaheer2018deepsets} or embeddings with self-attention to handle unsorted dice or opponent states\cite{vaswani-2017-attention}.
Additionally, \textrm{Yahtzee} could also be considered as a candidate environment for research into hierarchical reinforcement learning methods \cite{Barto2003}.
