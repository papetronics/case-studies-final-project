
\section{Discussion}
% <400-600 words>

\subsection{Is there a tradeoff between single-turn and full-game performance?}
Our results seem to show that there indeed is a tradeoff between the single-game and full-game objectives.
The single turn agent achieved high scores, often higher than the full-game REINFORCE agent,
but failed to learn a coherent bonus strategy, as expected.

At lower performance levels, there is a strong correlation between single-turn and full-game performance, as shown in Figure~\ref{fig:single-vs-full-game},
but as full-game performance improves, the correlation weakens. We were also able to use significantly smaller models for the single-turn agent,
indicating that the full-game task is inherently more complex.
However, it would be interesting to see if transfer learning from single-turn to full-game agents could help improve sample efficiency.

\subsection{Can an agent reach optimal play?}
Our best full-game agent, trained with A2C, was able to reach a median score of 241.78 points over 100,000 evaluation games,
which is within 5.0\% of the optimal DP score of 254.59.
This indicates that with appropriate algorithm and architectural choices, it is indeed possible to approach near-optimal performance using self-play alone.

\subsection{Which design choices most affect final performance?}
Our ablation studies highlighted several design choices that significantly impacted final performance.
First, the choice of RL algorithm and credit assignment was crucial; A2C with TD(0) outperformed both PPO and REINFORCE in terms of stability and final score.
Second, the state and action encodings played a significant role; specifically providing (rather than forcing the network to learn) some easily calculable features improved learning.
Third, entropy regularization played a huge role in stabilizing training and encouraging exploration. This made the most difference in the model's intra-turn strategy.
Lastly, reward shaping was critical to learning the upper section bonus strategy, agents learned it extremely slowly without it.

\subsection{What failure modes exist in learned policies?}
Despite the strong performance of the Actor-Critic agent, we observed several failure modes in the learned policies.
Most notably, the agent still overweighted the four-of-a-kind category, even when pursuing the upper section bonus would have been more beneficial.
Likewise, the agent did not make good use of the Chance category, using it right off the bat rather than saving it for difficult endgame scenarios.
These failures highlight persistent long-horizon credit-assignment and exploration challenges, despite the use of reward shaping and entropy regularization.

\subsection{Further Improvements}
Our final training runs used a fixed budget of 1 million games. This took nearly 10 hours to train on a single NVIDIA A100 GPU.
Samples were often the bottleneck for learning, in fact the bonus performance had not fully converged even after this many games.
Further improvements to parallelization and GPU utilization could help reduce training time, allowing for larger budgets to be explored.
Likewise, we struggled to get larger batch sizes to work, this would likely improve training times as well.

A few other improvements could likely help close the gap further. For example, different types of learning rate schedules could be used.
Soft-Actor-Critic (SAC) \cite{haarnoja2018softactorcriticoffpolicymaximum} style entropy control could be used to more finely adapt exploration during training,
we had to tune entropy coefficients manually for each algorithm, and they often required large adjustments when other hyperparameters changed.
