% ============================================================================
% 1. INTRODUCTION
% ============================================================================
% Use these to motivate "RL in complex stochastic games", "Yahtzee as a hard 
% combinatorial domain," and "deep RL works for big games, so let's try it here."

% ----------------------------------------------------------------------------
% 1.1 RL in Games / Long-Horizon Control
% ----------------------------------------------------------------------------

% Classic case study of TD learning beating humans in a highly stochastic board game (backgammon).
% Cite when motivating self-play + value function approximation in stochastic environments.
@article{tesauro1995tdgammon,
  title   = {Temporal Difference Learning and TD-Gammon},
  author  = {Tesauro, Gerald},
  journal = {Communications of the ACM},
  volume  = {38},
  number  = {3},
  pages   = {58--68},
  year    = {1995},
  doi     = {10.1145/203330.203343}
}

% Original AlphaGo paper: deep nets + MCTS, human data + self-play.
% Cite when introducing deep RL + tree search for board games or "superhuman Go with human data".
@article{silver2016alphago,
  title   = {Mastering the Game of Go with Deep Neural Networks and Tree Search},
  author  = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal = {Nature},
  volume  = {529},
  number  = {7587},
  pages   = {484--489},
  year    = {2016},
  doi     = {10.1038/nature16961}
}

% AlphaGo Zero: no human data, pure self-play, massive compute.
% Cite when emphasizing "learning from scratch" and strong self-play training regimes.
@article{silver2017alphagozero,
  title   = {Mastering the Game of Go without Human Knowledge},
  author  = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and others},
  journal = {Nature},
  volume  = {550},
  number  = {7676},
  pages   = {354--359},
  year    = {2017},
  doi     = {10.1038/nature24270}
}

% AlphaZero: single general algorithm for Go/Chess/Shogi via self-play.
% Cite when framing your method as "AlphaZero-style self-play for game X" or arguing for generality.
@article{silver2018alphazero,
  title   = {A General Reinforcement Learning Algorithm that Masters Chess, Shogi, and Go through Self-Play},
  author  = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
  journal = {Science},
  volume  = {362},
  number  = {6419},
  pages   = {1140--1144},
  year    = {2018},
  doi     = {10.1126/science.aar6404}
}

% DeepStack poker AI.
% Cite when discussing RL + search + value networks in high-variance, imperfect-information games (No-Limit Texas Hold'em).
@article{moravcik2017deepstack,
  title   = {DeepStack: Expert-Level Artificial Intelligence in Heads-Up No-Limit Poker},
  author  = {Morav{\v{c}}{\'\i}k, Matej and Schmid, Martin and Burch, Neil and Lis{\'y}, Viliam and Morrill, Dustin and Bard, Nolan and Davis, Trevor and Waugh, Kevin and Johanson, Michael and Bowling, Michael},
  journal = {Science},
  volume  = {356},
  number  = {6337},
  pages   = {508--513},
  year    = {2017},
  doi     = {10.1126/science.aam6960}
}

% Libratus poker AI.
% Cite when referencing game-theoretic RL / equilibrium-finding in complex stochastic games vs. human pros.
@article{brown2018libratus,
  title   = {Superhuman {AI} for Heads-Up No-Limit Poker: {Libratus} Beats Top Professionals},
  author  = {Brown, Noam and Sandholm, Tuomas},
  journal = {Science},
  volume  = {359},
  number  = {6374},
  pages   = {418--424},
  year    = {2018},
  doi     = {10.1126/science.aao1733}
}

% Hanabi challenge paper.
% Cite when motivating cooperative multi-agent RL, partial observability, or "hard stochastic card games".
@article{bard2020hanabi,
  title   = {The Hanabi Challenge: A New Frontier for {AI} Research},
  author  = {Bard, Nolan and Foerster, Jakob N. and Chandar, Sarath and Burch, Neil and Lanctot, Marc and Song, H. Francis and Parisotto, Emilio and Dumoulin, Vincent and Moitra, Subhodeep and Hughes, Edward and Dunning, Iain and Mourad, Shibl and Larochelle, Hugo and Bellemare, Marc G. and Bowling, Michael},
  journal = {Artificial Intelligence},
  volume  = {280},
  pages   = {103216},
  year    = {2020},
  doi     = {10.1016/j.artint.2019.103216}
}

% Early ADP/TD work on Tetris.
% Cite when talking about approximate dynamic programming on huge, stochastic MDPs (e.g., Tetris as a benchmark for instability/difficulty).
@techreport{bertsekas1996tetris,
  title       = {Temporal Differences-Based Policy Iteration and Applications in Neuro-Dynamic Programming},
  author      = {Bertsekas, Dimitri P. and Ioffe, Sergey},
  institution = {Laboratory for Information and Decision Systems, MIT},
  number      = {LIDS-P-2349},
  year        = {1996}
}

% "Tetris is actually hard but ADP can work" paper.
% Cite when you want a modern-ish RL/ADP reference on stochastic combinatorial games beyond Go/Atari (e.g., as evidence RL can crack tough puzzle-like domains).
@inproceedings{gabillon2013tetris,
  title     = {Approximate Dynamic Programming Finally Performs Well in the Game of Tetris},
  author    = {Gabillon, Victor and Ghavamzadeh, Mohammad and Lazaric, Alessandro and Scherrer, Bruno},
  booktitle = {Advances in Neural Information Processing Systems},
  volume    = {26},
  year      = {2013}
}

% Bootstrapped DQN for deep exploration.
% Cite when you need a canonical "exploration in deep RL" reference or when arguing about handling high stochasticity via better exploration.
@inproceedings{osband2016bootstrappeddqn,
  title     = {Deep Exploration via Bootstrapped {DQN}},
  author    = {Osband, Ian and Blundell, Charles and Pritzel, Alexander and Van Roy, Benjamin},
  booktitle = {Advances in Neural Information Processing Systems},
  volume    = {29},
  year      = {2016}
}

% ----------------------------------------------------------------------------
% 1.2 Yahtzee / This Domain Is Hard
% ----------------------------------------------------------------------------

% DP optimal strategy for Yahtzee
@techreport{glenn-2006-optimal-yahtzee,
  author      = {Jeffrey R. Glenn},
  title       = {An Optimal Strategy for Yahtzee},
  institution = {Loyola College in Maryland, Department of Computer Science},
  number      = {CS-TR-0002},
  year        = {2006},
  url         = {https://gunpowder.cs.loyola.edu/~jglenn/research/optimal_yahtzee.pdf}
}

@inproceedings{glenn-2007-solitaire-yahtzee,
  author    = {Jeffrey R. Glenn},
  title     = {Computer Strategies for Solitaire Yahtzee},
  booktitle = {2007 IEEE Symposium on Computational Intelligence and Games (CIG)},
  year      = {2007},
  pages     = {132--139},
  doi       = {10.1109/CIG.2007.368085}
}

@misc{verhoeff-1999-solitaire-yahtzee,
  author       = {Tom Verhoeff},
  title        = {Optimal Solitaire Yahtzee Strategies (slides)},
  year         = {1999},
  howpublished = {\url{https://www-set.win.tue.nl/~wstomv/misc/yahtzee/slides-2up.pdf}}
}

% Multiplayer Yahtzee complexity
@inproceedings{pawlewicz-2011-multiplayer-yahtzee,
  author    = {Pawlewicz, Jakub},
  editor    = {van den Herik, H. Jaap and Iida, Hiroyuki and Plaat, Aske},
  title     = {Nearly Optimal Computer Play in Multi-player Yahtzee},
  booktitle = {Computers and Games},
  year      = {2011},
  publisher = {Springer Berlin Heidelberg},
  address   = {Berlin, Heidelberg},
  pages     = {250--262},
  isbn      = {978-3-642-17928-0}
}

% Official game rules
@manual{hasbro-2022-yahtzee-rules,
  title        = {YAHTZEE Game: Instructions},
  author       = {{Hasbro, Inc.}},
  year         = {2022},
  note         = {Official rules and instructions},
  howpublished = {\url{https://instructions.hasbro.com/en-nz/instruction/yahtzee-game}}
}

% ----------------------------------------------------------------------------
% 1.3 Capacity / Network Size Motivation (optional but spicy)
% ----------------------------------------------------------------------------

% Talks about minimum and lower bounds on neural network size given game space.
@inproceedings{horne-1994-bounds-rnn-fsm,
  author    = {Horne, Bill G. and Hush, Don R.},
  title     = {Bounds on the Complexity of Recurrent Neural Network Implementations of Finite State Machines},
  booktitle = {Advances in Neural Information Processing Systems 6},
  editor    = {Cowan, Jack D. and Tesauro, Gerald and Alspector, Joshua},
  pages     = {359--366},
  year      = {1994},
  publisher = {Morgan Kaufmann},
  address   = {San Francisco, CA}
}

@article{hanin-2017-bounded-width-relu,
  author       = {Boris Hanin},
  title        = {Universal Function Approximation by Deep Neural Nets with Bounded Width and ReLU Activations},
  year         = {2017},
  eprint       = {arXiv:1708.02691},
  howpublished = {Mathematics 2019, 7(10), 992},
  doi          = {10.3390/math7100992}
}
