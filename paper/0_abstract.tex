\begin{abstract}
    Yahtzee is a classic dice game whose stochastic, combinatorial structure and delayed rewards make it an interesting mid-scale RL benchmark.
    While an optimal policy for solitaire Yahtzee can be computed using dynamic programming methods, multiplayer is intractable, motivating approximation methods.
    We formulate Yahtzee as a Markov Decision Process (MDP), and train self-play agents using various policy gradient methods:
    REINFORCE, Advantage Actor-Critic (A2C), and Proximal Policy Optimization (PPO), all using a multi-headed network with a shared trunk.
    We ablate feature encodings, architecture, return estimators, reward shaping, and entropy regularization.

    Under a fixed training budget, REINFORCE and PPO prove sensitive to hyperparameters and fail to reach near-optimal performance,
    whereas A2C trains robustly across a range of settings. Our agent attains a median score of <242.5> points over 10,000 games,
    within <4.7\%> of the optimal DP score of 254.59, achieving the upper section bonus and Yahtzee at rates of <X>\% and <Y>\%, respectively.
    All models struggle to learn the upper bonus strategy, overindexing on four-of-a-kind's, highlighting persistent long-horizon credit-assignment
    and exploration challenges.
\end{abstract}