%%% ALREADY USED, NEED TO CITE:

% Early practical discussion of TD-learning issues (stability, function approximation, etc.).
% Cite when talking about nuts-and-bolts TD, training details, and convergence concerns.
@article{tesauro1992practicaltd,
  title   = {Practical Issues in Temporal Difference Learning},
  author  = {Tesauro, Gerald},
  journal = {Machine Learning},
  volume  = {8},
  number  = {3-4},
  pages   = {257--277},
  year    = {1992},
  doi     = {10.1007/BF00992697}
}

% Temporal Differences
@article{sutton-1988-temporal-differences,
  author  = {Richard S. Sutton},
  title   = {Learning to Predict by the Methods of Temporal Differences},
  journal = {Machine Learning},
  year    = {1988},
  volume  = {3},
  number  = {1},
  pages   = {9--44},
  doi     = {10.1007/BF00115009}
}

% Reinforcement Learning for Combinatorial Optimization: A Survey
@article{mazyavkina2021rl_co_survey,
  title   = {Reinforcement Learning for Combinatorial Optimization: A Survey},
  author  = {Mazyavkina, Nina and Sviridov, Sergey and Ivanov, Sergei and Burnaev, Evgeny},
  journal = {Computers \& Operations Research},
  volume  = {134},
  pages   = {105400},
  year    = {2021},
  doi     = {10.1016/j.cor.2021.105400}
}

%%% MAYBE NEEDED, MAYBE NOT:

@inproceedings{haarnoja2018sac,
  title     = {Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor},
  author    = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning},
  series    = {Proceedings of Machine Learning Research},
  volume    = {80},
  pages     = {1861--1870},
  publisher = {PMLR},
  year      = {2018}
}

% KL-aware Policy Optimization Baselines (IMPALA, TRPO)
@article{espeholt2018impala,
  title   = {{IMPALA}: Scalable Distributed Deep-{RL} with Importance Weighted Actor-Learner Architectures},
  author  = {Espeholt, Lasse and Soyer, Hubert and Munos, R{\'e}mi and Simonyan, Karen
             and Mnih, Volodymyr and Ward, Tom and Doron, Yotam and Firoiu, Vlad
             and Harley, Tim and Dunning, Matthew and others},
  journal = {arXiv preprint arXiv:1802.01561},
  year    = {2018}
}

% Kaiming/He initialization for ReLU networks
@inproceedings{he-2015-delving-rectifiers,
  author    = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
  title     = {Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification},
  booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  year      = {2015},
  pages     = {1026--1034},
  doi       = {10.1109/ICCV.2015.123}
}

% SiLU (Sigmoid-Weighted Linear Units) activation for RL
@article{elfwing-2018-silu-rl,
  author  = {Stefan Elfwing and Eiji Uchibe and Kenji Doya},
  title   = {Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning},
  journal = {Neural Networks},
  year    = {2018},
  volume  = {107},
  pages   = {3--11},
  doi     = {10.1016/j.neunet.2017.12.012}
}

% GELU activation
@article{hendrycks-2016-gelu,
  author  = {Hendrycks, Dan and Gimpel, Kevin},
  title   = {Gaussian Error Linear Units (GELUs)},
  journal = {arXiv preprint arXiv:1606.08415},
  year    = {2016},
  url     = {https://arxiv.org/abs/1606.08415}
}

% RAdam optimizer (variance of adaptive learning rate)
@article{liu-2019-radam,
  title         = {On the Variance of the Adaptive Learning Rate and Beyond},
  author        = {Liu, Liyuan and Jiang, Haoming and He, Pengcheng and Chen, Weizhu and Liu, Xiaodong and Gao, Jianfeng and Han, Jiawei},
  journal       = {arXiv preprint arXiv:1908.03265},
  year          = {2019},
  eprint        = {1908.03265},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/1908.03265}
}

%%% PROBABLY NOT NEEDED, CAN DELETE LATER:

@inproceedings{cesa-bianchi2017boltzmann,
  title     = {Boltzmann Exploration Done Right},
  author    = {Cesa-Bianchi, Nicol{\`o} and Gentile, Claudio and Lugosi, G{\'a}bor and Neu, Gergely},
  booktitle = {Advances in Neural Information Processing Systems},
  volume    = {30},
  year      = {2017}
}

@article{goyal2017imagenet1hour,
  title   = {Accurate, Large Minibatch {SGD}: Training {ImageNet} in 1 Hour},
  author  = {Goyal, Priya and Doll{\'a}r, Piotr and Girshick, Ross and Noordhuis, Pieter
             and Wesolowski, Lukasz and Kyrola, Aleksi and Tulloch, Andrew
             and Jia, Yangqing and He, Kaiming},
  journal = {arXiv preprint arXiv:1706.02677},
  year    = {2017}
}



% AlphaZero: single general algorithm for Go/Chess/Shogi via self-play.
@article{silver2018alphazero,
  title   = {A General Reinforcement Learning Algorithm that Masters Chess, Shogi, and Go through Self-Play},
  author  = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
  journal = {Science},
  volume  = {362},
  number  = {6419},
  pages   = {1140--1144},
  year    = {2018},
  doi     = {10.1126/science.aar6404},
  url     = {https://www.science.org/doi/10.1126/science.aar6404},
  pdf     = {https://science.sciencemag.org/content/sci/362/6419/1140.full.pdf}
}

% Libratus poker AI.
% Cite when referencing game-theoretic RL / equilibrium-finding in complex stochastic games vs. human pros.
@article{brown2018libratus,
  title   = {Superhuman {AI} for Heads-Up No-Limit Poker: {Libratus} Beats Top Professionals},
  author  = {Brown, Noam and Sandholm, Tuomas},
  journal = {Science},
  volume  = {359},
  number  = {6374},
  pages   = {418--424},
  year    = {2018},
  doi     = {10.1126/science.aao1733}
}

% Hanabi challenge paper.
% Cite when motivating cooperative multi-agent RL, partial observability, or "hard stochastic card games".
@article{bard2020hanabi,
  title   = {The Hanabi Challenge: A New Frontier for {AI} Research},
  author  = {Bard, Nolan and Foerster, Jakob N. and Chandar, Sarath and Burch, Neil and Lanctot, Marc and Song, H. Francis and Parisotto, Emilio and Dumoulin, Vincent and Moitra, Subhodeep and Hughes, Edward and Dunning, Iain and Mourad, Shibl and Larochelle, Hugo and Bellemare, Marc G. and Bowling, Michael},
  journal = {Artificial Intelligence},
  volume  = {280},
  pages   = {103216},
  year    = {2020},
  doi     = {10.1016/j.artint.2019.103216}
}

% NFSP: deep RL + fictitious play for imperfect-information (poker-style) games.
% Cite when you mention RL in imperfect-information, highly stochastic settings or mixtures of RL + supervised best-response learning.
@article{heinrich2016nfsp,
  title         = {Deep Reinforcement Learning from Self-Play in Imperfect-Information Games},
  author        = {Heinrich, Johannes and Silver, David},
  journal       = {CoRR},
  volume        = {abs/1603.01121},
  year          = {2016},
  archiveprefix = {arXiv},
  eprint        = {1603.01121}
}