%
% CSML Final Project Paper
%

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2019}
\usepackage{times}
\usepackage{latexsym}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{float}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{pgfplots}
\pgfplotsset{compat=newest}
\usepackage{url}

\aclfinalcopy % Uncomment this line for the final submission

\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{Yahtzee: Reinforcement Learning Techniques for Stochastic Combinatorial Games}

\author{Nick Pape \\
  nap626 \\
  \texttt{nickpape@utexas.edu} \\}

\date{2025-12-01}

\begin{document}
\maketitle
\begin{abstract}
  Write your abstract here. This should be a concise summary of your work, 
  including the problem you're addressing, your approach, and key results.
  Keep it to about 150-250 words.
\end{abstract}

\section{Introduction}
% 600-900 words
% 297 words so far
\subsection{Yahtzee}
While on the surface \textit{Yahtzee} appears to be a trivial dice game \cite{hasbro-2022-yahtzee-rules}, it is actually a complex stochastic optimization problem with combinatorial complexity.
By it's nature, it is heavily driven by random chance due to dice rolls, but strategic decision-making is required to maximize expected score over the course of a game.
While the state complexity in solitaire play is large \cite{verhoeff-1999-solitaire-yahtzee} but manageable using dynamic programming techniques \cite{glenn-2007-solitaire-yahtzee}, adversarial multiplayer \textit{Yahtzee} remains an open problem due to the exponential growth of the state space with additional players \citet{pawlewicz-2011-multiplayer-yahtzee}. Even though Yahtzee has been a classic testbed for search and DP, it has been less explored as a deep reinforcement learning benchmark. Its varying, but well-defined,  problem settings make it a good candidate to serve as a "bridge" between smaller benchmarks like \textit{Lunar Lander} and large-scale games like \textit{Go} or \textit{Poker}. 

\subsection{Research Questions}
The primary research questions this paper aims to address are:
\begin{itemize}
    \item Can the tradeoff between maximizing the single-turn expected score and long-term game performance be quantified?
    \item Can a deep Reinforcement Learning agent trained only from self-play reach near-DP-optimal performance in full-game solitaire Yahtzee?
    \item Which design choices (state encoding, $\gamma$ schedule, entropy regularization, baselines, batch size, etc) most affect sample efficiency and final performance?
\end{itemize}

\subsection{Contributions}
The main contributions of this paper are:
\begin{itemize}
    \item A deep RL agent that achieves <X> average score, within <delta> points of an exact DP baseline under a strict budget of 1 million training games.
    \item An ablation study of key architectural and training choices and their impact on performance in Yahtzee.
    \item A failure mode analysis identifying issues in learned policies and their resolution through architectural or training modifications.
\end{itemize}

\section{Related Work}
% 600-800 words
% 742 words so far
\subsection{Complex Games}
Typical board and dice games have extreme state complexity or stochasticity; reinforcement learning methods are a natural fit for these problems.
In a classic example, \citet{tesauro1995tdgammon} utilized temporal difference learning to achieve superhuman performance in \textit{Backgammon}, another game with a large state space and stochastic elements.
Tetris has also been studied extensively; \citet{bertsekas1996tetris} utilized approximate dynamic programming methods to learn effective policies for the game.
Stochastic combinatorial games like Tetris can be effectively tackled using reinforcement learning methods \cite{gabillon2013tetris}.
Adversarial games with extremely large state spaces, such as Go were solved using Monte-Carlo Tree Search combined with deep value networks \cite{silver2016alphago}, and later work showed Go could be learned purely through self-play \cite{silver2017alphagozero}.
\citet{moravcik2017deepstack} demonstrated that \textit{Texas Hold'em}, a game with hidden information and stochasticity, could be also effectively learned.
It has been shown that highly stochastic games can be learned reliably, as long as methods to ensure better exploration are used \cite{osband2016bootstrappeddqn}. 

\subsection{DP Methods for Yahtzee}
Solitaire \textit{Yahtzee} is a complex game with an upper bound of $~7 \times 10^{15}$ possible states in its state space manifold, and has a high degree of stochasticity due to dice rolls being the primary driver of state transitions.
Despite this, it has been analytically solved using dynamic programming techniques \cite{verhoeff-1999-solitaire-yahtzee} and baseline metrics for optimal play are available, which calculates an ideal play score of $254.59$ points.
Later work by \citet{glenn-2006-optimal-yahtzee} optimizing the DP approach via symmetries to propose an efficient algorithm.
\citet{glenn-2007-solitaire-yahtzee} calculated the reachable state space to be $~5.3 \times 10^8$, and subsequently implemented this efficint algorithm, which and compared favorably to several benchmark heuristics.

However, adversarial Yahtzee remains an open problem.
While \citet{pawlewicz-2011-multiplayer-yahtzee} showed that DP techniques can be expanded to 2-player adversarial \textit{Yahtzee}, they do not scale to more players due to the exponential growth of the state space. Approximation methods must be utilized for larger player counts.

\subsection{Reinforcement Learning for Yahtzee}
Some prior attempts have been made to apply reinforcement learning to \textit{Yahtzee}.
YAMS attemped by Q-learning and SARSA, but was not able to surpass $~120$ points median \cite{belaich-2024-yams}.
For example, \citet{kang-2018-yahtzee-rl} applied heirarchical MAX-Q, achieving an average score of $129.58$ and a 67\% win-rate over a 1-turn expectimax agent baseline.
\citet{vasseur-2019-strategy-ladders} explored strategy ladders for multiplayer Yahtzee, to understand how sensitive Deep-Q networks were to the upper-bonus threshold.
Later, \cite{yuan-2023-two-player-yahtzee} applied Deep-Q networks to the adversarial setting.
Yahtzotron used heavy supervised pretraining and A2C to achieve an average of $~236$ points \cite{Haefner2021Yahtzotron}.
Although not formally written, \citet{DutschkeYahtzee} reports a deep-Q agent achieving a score of $241.6 \pm 40.7$ after just 8,000 games.

\subsection{Policy Gradient Methods and Variance Reduction}
Temporal Difference (TD) \cite{sutton-1988-temporal-differences} and REINFORCE \cite{williams-1992-reinforce} methods have long been used to assign credit and calculate training signals in long-horizon tasks.
These have spawned an entire hierarchy of policy gradient methods, including asynchronous advantage actor-critic (A3C) \cite{mnih-2016-a3c}, proximal policy optimization (PPO) \cite{schulman-2017-ppo}, and others \cite{sutton-2000-policy-gradient}. 

Unfortunately, all reinforcement learning methods struggle with long-horizon credit assignment due to the compounding variance of returns and sparse rewards, leading to high variance updates and unstable training \cite{bjorck-2022-high-variance}. There are numerous architectural and optimization tricks which can be employed to mitigate these issues.
For example, during training, the Adam optimizer \cite{kingma-2014-adam} is widely used due to its adaptive momentum term, which helps to stabilize high variance updates.
Performing "warmups" of the learning rate improves RL performance by deprioritizing policy updates during the poorly-performing early training regime \cite{kalra-2024-warmup} \cite{liu-2025-warmup-theory}.   
At initialization, Kaiming/He weight initialization \cite{he-2015-delving-rectifiers} has been shown to reduce variance in deep networks by normalizing activation variances across layers.
The SiLU and Swish activation functions \cite{ramachandran-2017-swish} are improvements of GELU \cite{hendrycks-2016-gelu} that have been shown to be highly effective in RL tasks; their smooth and fully differentiable curves improve gradient flow \cite{elfwing-2018-silu-rl}.

Another common problem with policy gradient methods is policy collapse due to premature convergence to suboptimal policies. <ENTROPY> <TEMPERATURE> <CLIPPING> <MONITORING ENTROPY>

\section{Problem Formulation}
% 400-600 words
\subsection{Game Description}\subsubsection{Rules of Yahtzee}
Yahtzee is played with five standard six-sided dice and a shared scorecard containing 13 categories.
Turns are rotated among players. A turn starts with a player rolling all five dice. They may then choose to keep
some dice, and re-roll the remaining ones. This process can be repeated up to two more times, for a total of three rolls.
After the final roll, the player must select one of the 13 scoring categories to apply to their current dice.
Each category has specific scoring rules, and each can only be used once per game.

\subsubsection{Mathematical Representation of Yahtzee}
The space of all possible dice configurations is:
$$\mathcal{D} \in \{1, 2, 3, 4, 5, 6\}^5$$
and the current state of the dice is represented as:
$$\mathbf{d} \in \mathcal{D}$$

In addition, we can represent the score card as a vector of length 13, where each element corresponds to a scoring category:

$$\mathbf{c} = (c_1, c_2, \ldots, c_{13}) \text{ where } c_i \in \mathcal{D}_i \cup \{\varnothing\}$$

and $\varnothing$ indicates an unused category.

Let us also define a dice face counting function which we can use to simplify score calculations:
\begin{align*}
n_v(\mathbf{d}) &= \sum_{i=1}^{5} \mathbb{I}(d_i = v),
\quad v \in \{1,\dots,6\} \\
\mathbf{n}(\mathbf{d}) &= \big(n_1(\mathbf{d}),\dots,n_6(\mathbf{d})\big)
\end{align*}

Let the potential score for each category be defined as follows (where detailed scoring rules can be found in Appendix):
\begin{align*}
\mathbf{f}(\mathbf{d}) &=
\bigl(f_1(\mathbf{d}), f_2(\mathbf{d}), \ldots, f_{13}(\mathbf{d})\bigr)
\end{align*}

The domain of possible scores in a score card for each category is then:

\begin{align*}
\mathcal{C}_i &= \Bigl\{i \cdot k : k \in \{0, 1, \ldots, 5\}\Bigr\} \forall i \in \{1, \ldots, 6\} \\
\mathcal{C}_7 &= \mathcal{C}_8 = \mathcal{C}_9 \\
              &= \{0, 1, 2, \ldots, 30\} \\
\mathcal{C}_9 &= \{0, 25\} \\
\mathcal{C}_{10} &= \{0, 30\} \\
\mathcal{C}_{11} &= \{0, 40\} \\
\mathcal{C}_{12} &= \{0, 50\}
\end{align*}

The current turn number can be represented as:
$$t \in \{1, 2, \ldots, 13\}, \quad t = \sum_{i=1}^{13} \mathbb{I}(c_i \neq \varnothing)$$

A single turn is composed of an initial dice roll, two optional re-rolls, and a final scoring decision.
Let $r = 0$, with $r \in \{0,1,2\}$ which is the number of rolls taken so far.
Prior to the first roll, the dice are randomized:
$$
\mathbf{d}_{r=0} \sim U(\mathcal{D})
$$

The player must decide which dice to keep and which to re-roll. Let the player define a keep vector:
$$\mathbf{k} \in \{0,1\}^5$$
where $\mathbf{k}_i = 1$ indicates that die $i$ is kept, otherwise it is re-rolled.

We can then define the transition of the dice state after a re-roll as:
\begin{align*}
\mathbf{d}' &\sim U(\mathcal{D}), \\[2pt]
\mathbf{d}_{r+1}
  &= (\mathbf{1} - \mathbf{k}) \odot \mathbf{d}'
   + \mathbf{k} \odot \mathbf{d}
\end{align*}


When $r=2$, the player must choose a scoring category to apply their current dice to. Define a scoring choice mask as a one-hot vector:
$$\mathbf{s} \in \{0,1\}^{13}, \quad \|\mathbf{s}|_1 = 1$$

For the purposes of calculating the final (or current) score, any field that has not been scored yet can be counted as zero.
We can define a mask vector for this:

\begin{align*}
&\mathbf{u}(\mathbf{c}) \in \{0,1\}^{13} \\
&\mathbf{u}(\mathbf{c})_i = \mathbb{I}\bigl(c_i \neq \varnothing\bigr), \quad \forall i = \{ 1, \ldots 13 \}
\end{align*}


If a player achieves a total score of 63 or more in the upper section (categories 1-6), they receive a bonus of 35 points:
$$
B(\mathbf{c}) = \begin{cases} 
  0, & 63 \leq \sum_{i=1}^{6} \mathbf{u}(\mathbf{c})_i \cdot \mathbf{c}_i \\
  35,  & \text{otherwise}
\end{cases}
$$

The player's score can thus be calculated as:
$$S(\mathbf{c}) = B(\mathbf{c}) + \big\langle \mathbf{u}(\mathbf{c}), \mathbf{c} \big\rangle$$

\subsection{MDP Formulation}
We can formulate Yahtzee as a Markov Decision Process (MDP) defined by the 4-tuple $(\mathcal{S}, \mathcal{A}, P, R, \gamma)$.

\subsection{Single-Turn Optimization Task}
\subsection{Full-Game Reinforcement Learning Task}

\section{Methodology}
1400-1700 words
\subsection{State Representation \& Input Features}
The design of $\mathbf{\phi}(\mathbf{s}) \rightarrow \mathbf{x}$ is one of the most critical components to the performance of a model \shortcite{sutton-2018-reinforcement-book}.

As such, several different representations were tested to evaluate their impact on learning efficiency and final performance.

\subsubsection{Dice Representation}
The dice representation can be encoded in several ways, depending on if we want to preserve permutation invariance or not.
Preserving ordering information (and implicitly, ranking) gives the model the benefit of being able to directly output actions corresponding to dice indices,
however, it comes at the cost of implicitly biasing the model to specific dice orderings; in other words, biasing towards a local optima of always keeping the highest ranking dice.
However, eliminating ordering information requires the model to either waste capacity learning permutation invariance or be architecturally supportive of it (e.g. with self-attention).
It also requires a different action representation, since actions can no longer correspond to specific dice indices. We attempted 5 different dice representations:
\begin{itemize}
  \item \textbf{One-hot encoding of ordered dice:} Each die is represented as a one-hot vector of length 6, and the 5 dice are concatenated in order.
  \item \textbf{Bin representation:} Here we pass through the dice face count vector $\mathbf{n}(\mathbf{d})$.
  \item \textbf{Combined representation:} Both ordered one-hot and bin representations are concatenated.
  \item \textbf{Learnable encoding of dice:} Each die is represented as a learnable embedding vector of length $d$, and self-attention is applied.
  \item \textbf{Learnable encoding of dice with positional encodings:} Similar to above, but with sinusoidal positional encodings added to each die embedding.
\end{itemize}    

\subsubsection{Scorecard Representation}
There are two important pieces of information $\mathbf{\phi}$ must encode about the scorecard: whether a category is open or closed,
and some form of progress towards the upper bonus. For the category availaboility, we simply pass through $\mathbf{c}$. Here, several different representations were tested:
\begin{itemize}
  \item \textbf{Raw upper score:} $\sum_{i \in \mathbf{c}} c_{i}$
  \item \textbf{Distance to bonus:} $63 - \sum_{i \in \mathbf{c}} c_{i}$
  \item \textbf{Norm. upper score:} $\min(\frac{1}{63} \sum_{i \in \mathbf{c}} c_{i}, 1)$
  \item \textbf{Distance to bonus:} $\mathbb{I}(\sum_{i \in \mathbf{c}} c_{i} \geq 63)$
  \item \textbf{"Golf" score:} 
\end{itemize}


\subsubsection{Computed Features}
\subsection{Neural Network Architecture}
\subsubsection{Trunk}
\subsubsection{Policy and Value Heads}
\subsubsection{Weight Initialization}
\subsubsection{Optimization \& Schedules}
\subsubsection{Training Metrics}
\subsection{Reinforcement Learning Algorithms}
\subsubsection{REINFORCE for Single-Turn Optimization}
\subsubsection{PPO for Full-Game Reinforcement Learning}
\subsection{Training Regimes}
\subsubsection{Single-Turn Training}
\subsubsection{Transfer Learning Setup}
\subsubsection{Evaluation Protocol}

\section{Results}

\subsection{Generalization of Single-Turn Agent to Full Game}
200-400 words

\subsection{Single-Turn Results \& Ablations}
600-700 words
\subsubsection{Baseline Model Performance}
\subsubsection{Representational Ablations}
\subsubsection{Architectural Ablations}
\subsubsection{Failure Mode Analysis}
\subsubsection{Summary}

\subsection{Full-Game Results}
600-700 words
\subsubsection{From-Scratch Training}
\subsubsection{REINFORCE}
\subsubsection{PPO}
\subsubsection{Transfer Learning}
\subsubsection{REINFORCE}
\subsubsection{PPO}
\subsubsection{Summary}

\subsection{Policy Analysis}
300-500 words
\subsubsection{Category Usage Distribution}
\subsubsection{Score Breakdown}
\subsubsection{Strategy Comparison}

\section{Discussion}
400-600 words

\section{Conclusion and Future Work}
200-300 words




\subsubsection{Neural Network Size Bounds}
One challenge in applying neural networks to approximate the value or policy function for Yahtzee is in 
determining an appropriate network size. We will quickly consider theoretical upper and lower bounds for the number of neurons required using
both the full state space and a reduced representation.

Assume for simplicity our reduced representation consists of the dice state $\mathbf{d}$ encoded one-hot,
the roll count $r$ encoded one-hot,
the bin count vector $\mathbf{n}(\mathbf{d})$ encoded one-hot,
and a score mask $\mathbf{u}(\mathbf{c})$. This gives input dimensionality of $30 + 3 + 30 + 13 = 52$ and cardinality of
$2^{52} \approx 4.5 \times 10^{15}$.


For a discrete control problem with a finite state size of $|\mathbf{S}|$, there are classical results \cite{horne-1994-bounds-rnn-fsm} 
showing that a recurrent neural network can represent any $|\mathbf{S}|$-state finite state machine with a number of neurons that 
grows only logarithmically with $|\mathbf{S}|$. This gives us an upper bound of $O(\sqrt{|\mathbf{S}|}) \approx 8.4 \times 10^{7}$ neurons.
However, this is a prohibitively large number of neurons for practical training and inference. 

For a lower bound, \citet{hanin-2017-bounded-width-relu} shows any continuous, convex function 
can in principle be approximated by a network whose hidden layer width has a lower bound of $\Omega(d+1)$, and a non-convex function
can be approximated by a network with a hidden layer width of at least $\Omega(d+3)$, where $d$ is the input dimension.
For a example, assume Yahtzee is non-convex, and we represent the dice state $\mathbf{d}$ and $r$ as one-hot encodings and utilize
the score mask $\mathbf{u}(\mathbf{c})$, then $x = 6 \cdot 5 + 3 + 13 = 46$ means our \textit{minimum} hidden layer width is 49 neurons. 

In practice, the ideal network size is not driven by the cardinality of the state space, but rather by the 

\section{AI Usage Disclosure}

This paper utilized artificial intelligence tools in the following ways:

\begin{itemize}
    \item \textbf{GitHub Copilot} was used for initial typesetting assistance with the LaTeX document structure and formatting.
    \item \textbf{ChatGPT GPT-5} was used for brainstorming ideas for reinforcement learning applications in games, helping to narrow down to a particular game that was both non-trivial and interesting for this research.
\end{itemize}

All other content, including research methodology, analysis, results interpretation, and conclusions, represents original work by the author. The AI tools were used only for initial ideation and technical formatting support, not for generating substantive content or analysis.

GitHub Copilot was also used to write this section.

\bibliography{1_introduction,2_related_work,3_methods,4_single_turn_results,5_full_game_experiments,6_discussion,7_appendix}
\bibliographystyle{acl_natbib}

\appendix

\section{Full Yahtzee Scoring Rules}
\label{app:scoring}
Next we define the indicator functions for each of the scoring categories:

\begin{align*}
\mathbb{I}_{3\mathrm{k}}(\mathbf{d})
&= \mathbb{I}\bigl\{ \max_{v} n_v(\mathbf{d}) \ge 3 \bigr\} \\[4pt]
\mathbb{I}_{4\mathrm{k}}(\mathbf{d})
&= \mathbb{I}\bigl\{ \max_{v} n_v(\mathbf{d}) \ge 4 \bigr\} \\[6pt]
\mathbb{I}_{\mathrm{full}}(\mathbf{d})
&= \mathbb{I}\Bigl\{
  \exists i, j \in \{1, \mathellipsis, 6 \} \ \text{with} \ n_i(\mathbf{d}) = 3 \land n_j(\mathbf{d}) = 2
\Bigr\} \\[6pt]
\mathbb{I}_{\mathrm{ss}}(\mathbf{d})
&= \mathbb{I}\Bigl\{
   \exists k \in \{1,2,3\} \ \text{with}\
   \sum_{v=k}^{k+3} \mathbb{I}\{n_v(\mathbf{d}) > 0\} = 4
   \Bigr\} \\[6pt]
\mathbb{I}_{\mathrm{ls}}(\mathbf{d})
&= \mathbb{I}\Bigl\{
   \exists k \in \{1,2\} \ \text{with}\
   \sum_{v=k}^{k+4} \mathbb{I}\{n_v(\mathbf{d}) > 0\} = 5
   \Bigr\} \\[6pt]
\mathbb{I}_{\mathrm{yahtzee}}(\mathbf{d})
  &= \mathbb{I}\bigl\{\max_v n_v(\mathbf{d}) = 5\bigr\}
\end{align*}

The potential score for each category can then be defined as:
\begin{align*}
f_j(\mathbf{d})   &= j \cdot n_j(\mathbf{d}), \qquad j \in \{1,\dots,6\} \\[4pt]
f_7(\mathbf{d})   &= \mathbf{1}^\top \mathbf{d} \cdot \mathbb{I}_{3\mathrm{k}}(\mathbf{d}) \\[4pt]
f_8(\mathbf{d})   &= \mathbf{1}^\top \mathbf{d} \cdot \mathbb{I}_{4\mathrm{k}}(\mathbf{d}) \\[4pt]
f_9(\mathbf{d})   &= 25 \cdot \mathbb{I}_{\mathrm{full}}(\mathbf{d}) \\[4pt]
f_{10}(\mathbf{d})&= 30 \cdot \mathbb{I}_{\mathrm{ss}}(\mathbf{d}) \\[4pt]
f_{11}(\mathbf{d})&= 40 \cdot \mathbb{I}_{\mathrm{ls}}(\mathbf{d}) \\[4pt]
f_{12}(\mathbf{d})&= 50 \cdot \mathbb{I}_{\mathrm{yahtzee}}(\mathbf{d}) \\[4pt]
f_{13}(\mathbf{d})&= \mathbf{1}^\top \cdot \mathbf{d} \\[4pt]
\mathbf{f}(\mathbf{d}) &=
\bigl(f_1(\mathbf{d}), f_2(\mathbf{d}), \ldots, f_{13}(\mathbf{d})\bigr)
\end{align*}

\end{document}
