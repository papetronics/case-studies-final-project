%
% CSML Final Project Paper
%

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2019}
\usepackage{times}
\usepackage{latexsym}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{float}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{pgfplots}
\pgfplotsset{compat=newest}
\usepackage{url}

\aclfinalcopy % Uncomment this line for the final submission

\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{Learning Yahtzee Using Deep Reinforcement Learning}

\author{Nick Pape \\
  nap626 \\
  \texttt{nickpape@utexas.edu} \\}

\date{2025-12-01}

\begin{document}
\maketitle
\begin{abstract}
  Write your abstract here. This should be a concise summary of your work, 
  including the problem you're addressing, your approach, and key results.
  Keep it to about 150-250 words.
\end{abstract}

\section{Introduction}
% 600-900 words
% 306 words so far
\subsection{Yahtzee}
While on the surface \textit{Yahtzee} appears to be a trivial dice game \cite{hasbro-2022-yahtzee-rules}, it is actually a complex stochastic optimization problem with combinatorial complexity.
By it's nature, it is heavily driven by random chance due to dice rolls, but strategic decision-making is required to maximize expected score over the course of a game.
While the state complexity in solitaire play is large \cite{verhoeff-1999-solitaire-yahtzee} but manageable using dynamic programming techniques \cite{glenn-2007-solitaire-yahtzee}, adversarial multiplayer \textit{Yahtzee} remains an open problem due to the exponential growth of the state space with additional players \citet{pawlewicz-2011-multiplayer-yahtzee}. Even though Yahtzee has been a classic testbed for search and DP, it has been less explored as a deep reinforcement learning benchmark. Its varying, but well-defined,  problem settings make it a good candidate to serve as a "bridge" between smaller benchmarks like \textit{Lunar Lander} and large-scale games like \textit{Go} or \textit{Poker}. 

\subsection{Research Questions}
The primary research questions this paper aims to address are:
\begin{itemize}
    \item Can the tradeoff between maximizing the single-turn expected score and long-term game performance be quantified?
    \item Can a deep Reinforcement Learning agent trained only from self-play reach near-DP-optimal performance in full-game solitaire Yahtzee?
    \item Which design choices (state encoding, $\gamma$ schedule, entropy regularization, baselines, batch size, etc) most affect sample efficiency and final performance?
\end{itemize}

\subsection{Contributions}
The main contributions of this paper are:
\begin{itemize}
    \item A deep RL agent that achieves <X> average score, within <delta> points of an exact DP baseline under a strict budget of 1 million training games.
    \item An ablation study of key architectural and training choices and their impact on performance in Yahtzee.
    \item A failure mode analysis identifying issues in learned policies and their resolution through architectural or training modifications.
\end{itemize}

\section{Related Work}
% 600-800 words
\subsection{Complex Games}
Typical board and dice games have extreme state complexity or stochasticity; reinforcement learning methods are a natural fit for these problems.
In a classic example, \citet{tesauro1995tdgammon} utilized temporal difference learning to achieve superhuman performance in \textit{Backgammon}, another game with a large state space and stochastic elements.
Tetris has also been studied extensively; \citet{bertsekas1996tetris} utilized approximate dynamic programming methods to learn effective policies for the game.
Stochastic combinatorial games like Tetris can be effectively tackled using reinforcement learning methods \cite{gabillon2013tetris}.
Adversarial games with extremely large state spaces, such as Go were solved using Monte-Carlo Tree Search combined with deep value networks \cite{silver2016alphago}, and later work showed Go could be learned purely through self-play \cite{silver2017alphagozero}.
\citet{moravcik2017deepstack} demonstrated that \textit{Texas Hold'em}, a game with hidden information and stochasticity, could be also effectively learned.
Highly stochastic games can be learned as well, utilizing methods to ensure better exploration \cite{osband2016bootstrappeddqn}. 

\subsection{Yahtzee}
Solitaire \textit{Yahtzee} is a complex game with over $7 \times 10^{15}$ possible states in its state space manifold, and has a high degree of stoachasticity due to dice rolls being the primary driver of state transitions.
Despite this, it has been analytically solved using dynamic programming techniques \cite{verhoeff-1999-solitaire-yahtzee} and baseline metrics for optimal play are available, which calculates an ideal play score of $254.59$ points.
Later work by \citet{glenn-2007-solitaire-yahtzee} improved upon this by optimizing the DP approach, utilizing symmetries and limiting analysis only to reachable states, allowing the problem to be solved on limited hardware, as well as describing several benchmark heuristics. \citet{glenn-2006-optimal-yahtzee}
However, adversarial Yahtzee remains an open problem.
While \citet{pawlewicz-2011-multiplayer-yahtzee} showed that DP techniques can be expanded to 2-player adversarial \textit{Yahtzee}, they do not scale to more players due to the exponential growth of the state space. Approximation methods must be utilized for larger player counts.

Some prior attempts have been made to apply reinforcement learning to \textit{Yahtzee}.
YAMS attemped by Q-learning and SARSA, but was not able to surpass $~120$ points median \cite{belaich-2024-yams}.
For example, \citet{kang-2018-yahtzee-rl} applied heirarchical MAX-Q, achieving an average score of $129.58$ and a 67\% win-rate over a 1-turn expectimax agent baseline.
\citet{vasseur-2019-strategy-ladders} explored strategy ladders for multiplayer Yahtzee, to understand how sensitive Deep-Q networks were to the upper-bonus threshold.
Later, \cite{yuan-2023-two-player-yahtzee} used Deep-Q networks and policy ladders to quantify the model's preference for prioritizing the upper-section bonus.
Yahtzotron used heavy supervised pretraining and A2C to achieve an average of $~236$ points \cite{Haefner2021Yahtzotron}.
Although not formally written, \citet{DutschkeYahtzee} reports a deep-Q agent achieving a score of $241.6 \pm 40.7$ after just 8,000 games.

\subsection{Long-Horizon Policy Gradient Methods}


\subsection{Architecture and Optimization Components}

\section{Problem Formulation}
% 400-600 words
\subsection{Game Description}
\subsection{Single-Turn Optimization Task}
\subsection{Full-Game Reinforcement Learning Task}

\section{Methodology}
% 1400-1700 words
\subsection{State Representation \& Input Features}
\subsubsection{Dice Representation}
\subsubsection{Scorecard Representation}
\subsubsection{Computed Features}
\subsection{Neural Network Architecture}
\subsubsection{Trunk}
\subsubsection{Policy and Value Heads}
\subsubsection{Weight Initialization}
\subsubsection{Optimization \& Schedules}
\subsection{Reinforcement Learning Algorithms}
\subsubsection{REINFORCE for Single-Turn Optimization}
\subsubsection{PPO for Full-Game Reinforcement Learning}
\subsection{Training Regimes}
\subsubsection{Single-Turn Training}
\subsubsection{Transfer Learning Setup}
\subsubsection{Evaluation Protocol}


\section{Single-Turn Results \& Ablations}
% 700-900 words
\subsection{Baseline Model Performance}
\subsection{Representational Ablations}
\subsection{Architectural Ablations}
\subsection{Failure Mode Analysis}
\subsection{Summary}

\section{Full-Game Results}
% 700-900 words
\subsection{From-Scratch Training}
\subsection{REINFORCE}
\subsection{PPO}
\subsection{Transfer Learning}
\subsection{REINFORCE}
\subsection{PPO}
\subsection{Summary}

\section{Policy Analysis}
% 300-500 words
\subsection{Category Usage Distribution}
\subsection{Score Breakdown}
\subsection{Strategy Comparison}

\section{Discussion}
% 400-600 words

\section{Conclusion and Future Work}
% 200-300 words



\section{Introduction and Research Background}

At first glance, \textit{Yahtzee} appears to be a simple dice game. Simply roll some dice, choose which to keep,
and score points based on the resulting combinations. While arithmetic may be the minimum skill required to play,
playing the game well requires strategic decision-making, probabilistic reasoning, and risk management.

In this paper, we explore the application of various reinforcement learning techniques to optimize gameplay in Yahtzee.
We investigate how different reinforcement learning algorithms, model architectures, state and action representations,
and reward structures impact the performance of agents in this complex environment.

\subsection{Background}

\subsubsection{Rules of Yahtzee}
Yahtzee is played with five standard six-sided dice and a shared scorecard containing 13 categories.
Turns are rotated among players. A turn starts with a player rolling all five dice. They may then choose to keep
some dice, and re-roll the remaining ones. This process can be repeated up to two more times, for a total of three rolls.
After the final roll, the player must select one of the 13 scoring categories to apply to their current dice.
Each category has specific scoring rules, and each can only be used once per game.

\subsubsection{Mathematical Representation of Yahtzee}
The space of all possible dice configurations is:
$$\mathcal{D} \in \{1, 2, 3, 4, 5, 6\}^5$$
and the current state of the dice is represented as:
$$\mathbf{d} \in \mathcal{D}$$

In addition, we can represent the score card as a vector of length 13, where each element corresponds to a scoring category:

$$\mathbf{c} = (c_1, c_2, \ldots, c_{13}) \text{ where } c_i \in \mathcal{D}_i \cup \{\varnothing\}$$

and $\varnothing$ indicates an unused category.

Let us also define a dice face counting function which we can use to simplify score calculations:
\begin{align*}
n_v(\mathbf{d}) &= \sum_{i=1}^{5} \mathbb{I}(d_i = v),
\quad v \in \{1,\dots,6\} \\
\mathbf{n}(\mathbf{d}) &= \big(n_1(\mathbf{d}),\dots,n_6(\mathbf{d})\big)
\end{align*}

Let the potential score for each category be defined as follows (where detailed scoring rules can be found in Appendix):
\begin{align*}
\mathbf{f}(\mathbf{d}) &=
\bigl(f_1(\mathbf{d}), f_2(\mathbf{d}), \ldots, f_{13}(\mathbf{d})\bigr)
\end{align*}

The domain of possible scores in a score card for each category is then:

\begin{align*}
\mathcal{C}_i &= \Bigl\{i \cdot k : k \in \{0, 1, \ldots, 5\}\Bigr\} \forall i \in \{1, \ldots, 6\} \\
\mathcal{C}_7 &= \mathcal{C}_8 = \mathcal{C}_9 \\
              &= \{0, 1, 2, \ldots, 30\} \\
\mathcal{C}_9 &= \{0, 25\} \\
\mathcal{C}_{10} &= \{0, 30\} \\
\mathcal{C}_{11} &= \{0, 40\} \\
\mathcal{C}_{12} &= \{0, 50\}
\end{align*}

The current turn number can be represented as:
$$t \in \{1, 2, \ldots, 13\}, \quad t = \sum_{i=1}^{13} \mathbb{I}(c_i \neq \varnothing)$$

A single turn is composed of an initial dice roll, two optional re-rolls, and a final scoring decision.
Let $r = 0$, with $r \in \{0,1,2\}$ which is the number of rolls taken so far.
Prior to the first roll, the dice are randomized:
$$
\mathbf{d}_{r=0} \sim U(\mathcal{D})
$$

The player must decide which dice to keep and which to re-roll. Let the player define a keep vector:
$$\mathbf{k} \in \{0,1\}^5$$
where $\mathbf{k}_i = 1$ indicates that die $i$ is kept, otherwise it is re-rolled.

We can then define the transition of the dice state after a re-roll as:
\begin{align*}
\mathbf{d}' &\sim U(\mathcal{D}), \\[2pt]
\mathbf{d}_{r+1}
  &= (\mathbf{1} - \mathbf{k}) \odot \mathbf{d}'
   + \mathbf{k} \odot \mathbf{d}
\end{align*}


When $r=2$, the player must choose a scoring category to apply their current dice to. Define a scoring choice mask as a one-hot vector:
$$\mathbf{s} \in \{0,1\}^{13}, \quad \|\mathbf{s}|_1 = 1$$

For the purposes of calculating the final (or current) score, any field that has not been scored yet can be counted as zero.
We can define a mask vector for this:

\begin{align*}
&\mathbf{u}(\mathbf{c}) \in \{0,1\}^{13} \\
&\mathbf{u}(\mathbf{c})_i = \mathbb{I}\bigl(c_i \neq \varnothing\bigr), \quad \forall i = \{ 1, \ldots 13 \}
\end{align*}


If a player achieves a total score of 63 or more in the upper section (categories 1-6), they receive a bonus of 35 points:
$$
B(\mathbf{c}) = \begin{cases} 
  0, & 63 \leq \sum_{i=1}^{6} \mathbf{u}(\mathbf{c})_i \cdot \mathbf{c}_i \\
  35,  & \text{otherwise}
\end{cases}
$$

The player's score can thus be calculated as:
$$S(\mathbf{c}) = B(\mathbf{c}) + \big\langle \mathbf{u}(\mathbf{c}), \mathbf{c} \big\rangle$$

\subsection{Complexity Analysis}

\subsubsection{State Space Analysis}

The unfiltered state space for a solitaire game of Yahtzee can be described by the tuple of the current dice configuration,
the scorecard status, and the roll count within the turn:

$$\mathbf{S} = (\mathbf{d}, \mathbf{c}, r)$$

The cardinality of the state space is primarily driven by the score card, $\mathcal{C}$:
$$|\mathcal{C}| = 7^6 \cdot 32^3 \cdot 3^4 \approx 3.12 \times 10^{11}$$

However the dice configuration space is also significant:
$$|\mathcal{D}| = 6^5 = 7776$$

And the roll count contributes a factor of 3:
$$|r \in \{ 0, 1, 2 \}| = 3$$

Thus the unfiltered state space has a cardinality of:
\begin{align*}
|\mathbf{S}| &= |\mathcal{D}| \cdot |\mathcal{C}| \cdot |r| \\
&= 6^5 \cdot 7^6 \cdot 32^3 \cdot 3^4 \cdot 3 \\
&\approx 7.28 \times 10^{15}
\end{align*}

However, 

\subsubsection{Infeasibility of Tabular Methods}

If this space were to be solved with tabular dynamic programming techniques using 16-bit half-precision floats, it would require nearly 1.5 petabytes of memory to store. 
While there have been prior attempts to use tabular methods for Yahtzee, these required significant optimizations and clustered computing resources to
solve directly \cite{}.

For a local-play setting, this makes traditional tabular methods infeasible for Yahtzee, necessitating the use of function approximation methods such as deep neural networks.

\subsubsection{Neural Network Size Bounds}
One challenge in applying neural networks to approximate the value or policy function for Yahtzee is in 
determining an appropriate network size. We will quickly consider theoretical upper and lower bounds for the number of neurons required using
both the full state space and a reduced representation.

Assume for simplicity our reduced representation consists of the dice state $\mathbf{d}$ encoded one-hot,
the roll count $r$ encoded one-hot,
the bin count vector $\mathbf{n}(\mathbf{d})$ encoded one-hot,
and a score mask $\mathbf{u}(\mathbf{c})$. This gives input dimensionality of $30 + 3 + 30 + 13 = 52$ and cardinality of
$2^{52} \approx 4.5 \times 10^{15}$.


For a discrete control problem with a finite state size of $|\mathbf{S}|$, there are classical results \cite{horne-1994-bounds-rnn-fsm} 
showing that a recurrent neural network can represent any $|\mathbf{S}|$-state finite state machine with a number of neurons that 
grows only logarithmically with $|\mathbf{S}|$. This gives us an upper bound of $O(\sqrt{|\mathbf{S}|}) \approx 8.4 \times 10^{7}$ neurons.
However, this is a prohibitively large number of neurons for practical training and inference. 

For a lower bound, \citet{hanin-2017-bounded-width-relu} shows any continuous, convex function 
can in principle be approximated by a network whose hidden layer width has a lower bound of $\Omega(d+1)$, and a non-convex function
can be approximated by a network with a hidden layer width of at least $\Omega(d+3)$, where $d$ is the input dimension.
For a example, assume Yahtzee is non-convex, and we represent the dice state $\mathbf{d}$ and $r$ as one-hot encodings and utilize
the score mask $\mathbf{u}(\mathbf{c})$, then $x = 6 \cdot 5 + 3 + 13 = 46$ means our \textit{minimum} hidden layer width is 49 neurons. 

In practice, the ideal network size is not driven by the cardinality of the state space, but rather by the 

\subsection{Intra-turn Decision Complexity}
Test.

\subsection{Problem Statement}
Clearly state the problem you are addressing.

\subsection{Research Objectives}
Outline your research objectives and what you aim to achieve.

\section{Research and Methods}

Describe your approach and methods here.

\subsection{Methodology Overview}
Provide an overview of your research methodology and approach.

\subsection{Model Architecture}
Describe your model or approach in detail.

\subsection{Experimental Design}
Describe your experimental design and evaluation metrics.

\section{Materials and Data Sources}

Describe the dataset and materials you are using.

\subsection{Dataset Description}
Provide detailed information about your dataset, including size, characteristics, and source.

\subsection{Data Collection and Preprocessing}
Explain how data was collected and any preprocessing steps performed.

\subsection{Tools and Technologies}
List the tools, frameworks, and technologies used in your research.

\section{Results}

Present your experimental results here.

\begin{table}[H]
\begin{center}
\begin{tabular}{|l|c|c|}
\hline \textbf{Method} & \textbf{Metric 1} & \textbf{Metric 2} \\ \hline
Baseline & XX.XX & XX.XX \\
Your Method & XX.XX & XX.XX \\
\hline
\end{tabular}
\end{center}
\caption{\label{results-table} Results comparison}
\end{table}


\section{Discussion and Conclusion}

\subsection{Discussion of Results}
Discuss your findings, their implications, and how they relate to existing literature.

\subsection{Limitations}
Acknowledge the limitations of your study and approach.

\subsection{Future Work}
Discuss potential directions for future research.

\subsection{Conclusion}
Summarize your work and its contributions to the field.

\section{AI Usage Disclosure}

This paper utilized artificial intelligence tools in the following ways:

\begin{itemize}
    \item \textbf{GitHub Copilot} was used for initial typesetting assistance with the LaTeX document structure and formatting.
    \item \textbf{ChatGPT GPT-5} was used for brainstorming ideas for reinforcement learning applications in games, helping to narrow down to a particular game that was both non-trivial and interesting for this research.
\end{itemize}

All other content, including research methodology, analysis, results interpretation, and conclusions, represents original work by the author. The AI tools were used only for initial ideation and technical formatting support, not for generating substantive content or analysis.

GitHub Copilot was also used to write this section.

\section{Acknowledgments}

Acknowledge any help or resources you used.

\bibliography{1_introduction,2_related_work,3_methods,4_single_turn_results,5_full_game_experiments,6_discussion,7_appendix}
\bibliographystyle{acl_natbib}

\appendix

\section{Full Yahtzee Scoring Rules}
\label{app:scoring}
Next we define the indicator functions for each of the scoring categories:

\begin{align*}
\mathbb{I}_{3\mathrm{k}}(\mathbf{d})
&= \mathbb{I}\bigl\{ \max_{v} n_v(\mathbf{d}) \ge 3 \bigr\} \\[4pt]
\mathbb{I}_{4\mathrm{k}}(\mathbf{d})
&= \mathbb{I}\bigl\{ \max_{v} n_v(\mathbf{d}) \ge 4 \bigr\} \\[6pt]
\mathbb{I}_{\mathrm{full}}(\mathbf{d})
&= \mathbb{I}\Bigl\{
  \exists i, j \in \{1, \mathellipsis, 6 \} \ \text{with} \ n_i(\mathbf{d}) = 3 \land n_j(\mathbf{d}) = 2
\Bigr\} \\[6pt]
\mathbb{I}_{\mathrm{ss}}(\mathbf{d})
&= \mathbb{I}\Bigl\{
   \exists k \in \{1,2,3\} \ \text{with}\
   \sum_{v=k}^{k+3} \mathbb{I}\{n_v(\mathbf{d}) > 0\} = 4
   \Bigr\} \\[6pt]
\mathbb{I}_{\mathrm{ls}}(\mathbf{d})
&= \mathbb{I}\Bigl\{
   \exists k \in \{1,2\} \ \text{with}\
   \sum_{v=k}^{k+4} \mathbb{I}\{n_v(\mathbf{d}) > 0\} = 5
   \Bigr\} \\[6pt]
\mathbb{I}_{\mathrm{yahtzee}}(\mathbf{d})
  &= \mathbb{I}\bigl\{\max_v n_v(\mathbf{d}) = 5\bigr\}
\end{align*}

The potential score for each category can then be defined as:
\begin{align*}
f_j(\mathbf{d})   &= j \cdot n_j(\mathbf{d}), \qquad j \in \{1,\dots,6\} \\[4pt]
f_7(\mathbf{d})   &= \mathbf{1}^\top \mathbf{d} \cdot \mathbb{I}_{3\mathrm{k}}(\mathbf{d}) \\[4pt]
f_8(\mathbf{d})   &= \mathbf{1}^\top \mathbf{d} \cdot \mathbb{I}_{4\mathrm{k}}(\mathbf{d}) \\[4pt]
f_9(\mathbf{d})   &= 25 \cdot \mathbb{I}_{\mathrm{full}}(\mathbf{d}) \\[4pt]
f_{10}(\mathbf{d})&= 30 \cdot \mathbb{I}_{\mathrm{ss}}(\mathbf{d}) \\[4pt]
f_{11}(\mathbf{d})&= 40 \cdot \mathbb{I}_{\mathrm{ls}}(\mathbf{d}) \\[4pt]
f_{12}(\mathbf{d})&= 50 \cdot \mathbb{I}_{\mathrm{yahtzee}}(\mathbf{d}) \\[4pt]
f_{13}(\mathbf{d})&= \mathbf{1}^\top \cdot \mathbf{d} \\[4pt]
\mathbf{f}(\mathbf{d}) &=
\bigl(f_1(\mathbf{d}), f_2(\mathbf{d}), \ldots, f_{13}(\mathbf{d})\bigr)
\end{align*}

\end{document}
