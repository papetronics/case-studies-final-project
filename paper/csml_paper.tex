%
% CSML Final Project Paper
%

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2019}
\usepackage{times}
\usepackage{latexsym}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{float}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{pgfplots}
\usepackage[inline]{enumitem} % note the [inline]
\pgfplotsset{compat=newest}
\usepackage{url}

\aclfinalcopy % Uncomment this line for the final submission

\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{Yahtzee: Reinforcement Learning Techniques for Stochastic Combinatorial Games}

\author{Nick Pape \\
  nap626 \\
  \texttt{nickpape@utexas.edu} \\}

\date{2025-12-01}

\begin{document}
\maketitle
\begin{abstract}
  Write your abstract here. This should be a concise summary of your work,
  including the problem you're addressing, your approach, and key results.
  Keep it to about 150-250 words.
\end{abstract}

\section{Introduction}
% 600-900 words
% 513 words so far
\subsection{Yahtzee as a Reinforcement Learning Benchmark}
While on the surface \textit{Yahtzee} appears to be a trivial dice game \cite{hasbro-2022-yahtzee-rules}, it is actually a complex stochastic optimization problem with combinatorial complexity.

Although there are methods for computing optimal play in \textit{Yahtzee} using dynamic programming, these are computationally expensive and do not scale well to multiplayer settings.
\textit{Yahtzee} offers a rich environment for testing reinforcement learning (RL) solutions due to its combination of a large but manageable state space, randomness, ease of simulation, subtle strategic considerations, and easily identifiable subproblems.
While there have been a small number of efforts to create RL agents for \textit{Yahtzee}, a comprehensive approach using self-play has yet to be published.
It remains an open question of whether deep RL methods can approach optimal performance in full-game \textit{Yahtzee}, and which architectural and training choices most affect learning efficiency and final performance.
Similarly, a robust solution for multiplayer \textit{Yahtzee} using RL methods has yet to be demonstrated.

\textit{Yahtzee} is an ideal candidate to serve as a bridge between simple toy problems such as \textit{Lunar Lander} \cite{brockman2016openai} and extremely complex games like Go \cite{silver2016alphago}.
Typical small benchmarks often offer low stochasticity and simple combinatorics whereas complex games have intractable state spaces and require massive computational resources and heavy engineering to solve.
\textit{Yahtzee} sits in a middle ground where an analytic optimum exists, but reaching it with RL methods is non-trivial.
Its blend of stochasticity and large state space makes it a challenging yet feasible benchmark for RL research.

\subsection{Objectives}

In this paper we aim to methodically study whether a deep RL agent can achieve near DP-optimal performance in full-game solitaire \textit{Yahtzee} using only self-play without any shaping rewards or expert demonstrations,
and how architectural and training choices affect learning efficiency.

Concretely, we ask: \begin{enumerate*}[label=(\roman*)]
  \item How does the trade-off between maximizing single-turn expected score and full-game performance behave?
  \item Can a shaping-free self-play agent can reach optimal performance under a fixed training budget?
  \item Which design choices (state and action encodings, variance controls, baselines, entropy, etc) most affect final performance?
  \item What failure modes exist in learned policies and how can they be addressed?
  \item Can we find a successful architecture which could be adapted to multiplayer \textit{Yahtzee}?
\end{enumerate*}

\section{Related Work}
% 600-800 words
% 742 words so far

\subsection{Policy Gradient Methods and Variance Reduction}
Policy-gradient methods are a family of algorithms which directly optimize a parameterized policy $\pi_{\theta}$ to follow an estimate of the performance gradient \cite{sutton-2000-policy-gradient}.
A simple formulation of this is the REINFORCE algorithm \cite{williams-1992-reinforce}, which uses Monte-Carlo returns $G_{t}^{MC}$ on finite, episodic tasks; however, while unbiased, it suffers from high variance.
Actor-critic methods \cite{konda-1999-actorcritic, mnih-2016-a3c} address this by learning a seperate value function, the critic $V_{\phi}$, which serves a variance reducing baseline.
A recent and popular algorithm, proximal policy optimization (PPO) \cite{schulman-2017-ppo}, utilizes a clipped objective $L^{CLIP}(\theta)$
and explicit Kulback-Leibler (KL) divergence control to dramatically reduce variance and ensure stable updates.

In long episodic games, the choice of return calculation affects sample efficiency, bias, and variance. Monte-Carlo (MC) returns $G_{t}^{MC}$ use a summation over the full series of rewards until the end of the episode.
This approach is unbiased but has high variance.
Temporal Difference returns calculate the TD-error term $\delta_t$ using a bootstrapped estimate for the value of the next state \cite{sutton-1988-temporal-differences}, which is biased but has lower variance.
n-step returns $G_{t}^{TD(n)}$ \cite{sutton-2018-reinforcement-book} interpolate between MC and single-step TD returns, allowing us to define a time horizon $n$ over which to sum rewards before bootstrapping, this lets us manually control the bias-variance tradeoff.
This is improved by the $TD(\lambda)$ algorithm \cite{sutton-2018-reinforcement-book}, which uses an exponentially weighted average of n-step returns, effectively blending multiple time horizons.
Actor-Critic and PPO can use any of these return estimation methods, including the generalized advantage estimate (GAE) \cite{schulman-2016-gae}, which is a special formulation of $TD(\lambda)$.

Aside from return estimation, there is a host of other variance reduction techniques which can be employed for policy gradient methods.
Subtracting a learned baseline from REINFORCE yields an advantage estimate that reduces variance without changing its expectation \cite{weaver2013optimal, greensmith-2004-variance-reduction}.
Normalizing advantages across a batch improves gradient conditioning and is common practice.
Entropy regularization prevents early collapse to suboptimal policies by encouraging exploration via the addition of an explicit entropy bonus term in the loss function \cite{williams-peng-1991-function-optimization}.
Gradient clipping is frequently used alongside these techniques to stop rare, but large, gradient updates from destabilizing training \cite{pascanu-2013-rnn-clipping}.
While high variance is unavoidable in deep reinforcement learning, poor performance can often be linked to numerical instability rather than inherent flaws in algorithmic design \cite{bjorck-2022-high-variance};
simple tweaks like normalizing features before activations can dramatically improve stability.

\subsection{Complex Games}
Typical board and dice games have extreme state complexity or stochasticity; reinforcement learning methods are a natural fit for these problems.
In a classic example, \citet{tesauro1995tdgammon} utilized temporal difference learning to achieve superhuman performance in \textit{Backgammon}, another game with a large state space and stochastic elements.
Tetris, which is deterministic but combinatorial, has also been studied extensively; \citet{bertsekas1996tetris} utilized approximate dynamic programming methods to learn effective policies for the game,
while \citet{gabillon2013tetris} effectively tackled the game using reinforcement learning methods.
\citet{moravcik2017deepstack} demonstrated that \textit{Texas Hold'em}, a stochastic game with hidden information, could be effectively learned.
Many other stochastic games can be learned we well, so long as methods which ensure better exploration are used \cite{osband2016bootstrappeddqn}.
Lastly, RL methods can be used to reach superhuman performance on adversarial games, even despite their sparse reward structures.
For example, the game of Go, which has a notoriously intractable state space was solved using Monte-Carlo Tree Search and deep value networks \cite{silver2016alphago}.
Subsequent work showed Go could be learned without the use of expert data, purely through self-play \cite{silver2017alphagozero}.
In total, these works establish that RL methods can handle highly stochastic, combinatorial games, suggesting that \textit{Yahtzee} is a natural but underexplored candidate in this family.

\subsection{DP Methods for Yahtzee}
Solitaire \textit{Yahtzee} is a complex game with an upper bound of $~7 \times 10^{15}$ possible states in its state space.
It has a high degree of stochasticity, as dice rolls are the primary driver of state transitions.
Despite this, it has been analytically solved using dynamic programming techniques; \citet{verhoeff-1999-solitaire-yahtzee}, calculated an that the average score achieved during ideal play is $254.59$ points, which serves as the gold-standard baselline for solitaire \textit{Yahtzee}.
Later work by \citet{glenn-2006-optimal-yahtzee} optimized the DP approach via symmetries to propose a more efficient algorithm for computing the optimal policy, with a reachable state space of $~5.3 \times 10^8$ states \cite{glenn-2007-solitaire-yahtzee}.

However, adversarial \textit{Yahtzee} remains an open problem.
While \citet{pawlewicz-2011-multiplayer-yahtzee} showed that DP techniques can be expanded to 2-player adversarial \textit{Yahtzee}, they do not scale to more players due to the exponential growth of the state space.
Approximation methods must be utilized for larger player counts.

\subsection{Reinforcement Learning for Yahtzee}
Some prior attempts have been made to apply reinforcement learning to \textit{Yahtzee}.
YAMS attempted used Q-learning and SARSA to attempt to learn \textit{Yahtzee}, but was not able to surpass $~120$ points median \cite{belaich-2024-yams}.
Likewise, \citet{kang-2018-yahtzee-rl} applied hierarchical MAX-Q, achieving an average score of $129.58$ and a 67\% win-rate over a 1-turn expectimax agent baseline.
\citet{vasseur-2019-strategy-ladders} explored strategy ladders for multiplayer \textit{Yahtzee}, to understand how sensitive Deep-Q networks were to the upper-bonus threshold.
Later, \cite{yuan-2023-two-player-yahtzee} applied Deep-Q networks to the adversarial setting.

Additionally, some recent informal work has reported success using RL methods for \textit{Yahtzee}.
For example, Yahtzotron used heavy supervised pretraining and A2C to achieve an average of $~236$ points \cite{Haefner2021Yahtzotron}.
Although it is unclear if the results are repeatable, \citet{DutschkeYahtzee} reports a deep-Q agent achieving a score of $241.6 \pm 40.7$ after just 8,000 games.

No prior work systemically explores policy gradient methods with variance reduction tricks on full-game solitaire \textit{Yahtzee}, attempts transfer learning from single-turn optimization, provides a detailed ablation and failure mode analysis,
or offers an architecture that is theoretically transferrable to multiplayer settings.


\section{Problem Formulation}
% 400-600 words
\subsection{Game Description}
\subsubsection{Rules of Yahtzee}
\textit{Yahtzee} is played with five standard six-sided dice and a shared scorecard containing 13 categories.
Turns are rotated among players. A turn starts with a player rolling all five dice. They may then choose to keep
some dice, and re-roll the remaining ones. This process can be repeated up to two more times, for a total of three rolls.
After the final roll, the player must select one of the 13 scoring categories to apply to their current dice.
Each category has specific scoring rules, and each can only be used once per game.

\subsubsection{Mathematical Representation of Yahtzee}
\label{sec:yahtzee-definitions}
The space of all possible dice configurations is:
$$\mathcal{D} \in \{1, 2, 3, 4, 5, 6\}^5$$
and the current state of the dice is represented as:
$$\mathbf{d} \in \mathcal{D}$$

In addition, we can represent the score card as a vector of length 13, where each element corresponds to a scoring category:

$$\mathbf{c} = (c_1, c_2, \ldots, c_{13}) \text{ where } c_i \in \mathcal{D}_i \cup \{\varnothing\}$$

and $\varnothing$ indicates an unused category.

Let us also define a dice face counting function which we can use to simplify score calculations:
\begin{align*}
  n_v(\mathbf{d})        & = \sum_{i=1}^{5} \mathbb{I}(d_i = v),
  \quad v \in \{1,\dots,6\}                                                  \\
  \mathbf{n}(\mathbf{d}) & = \big(n_1(\mathbf{d}),\dots,n_6(\mathbf{d})\big)
\end{align*}

Let the potential score for each category be defined as follows (where detailed scoring rules can be found in Appendix~\ref{app:scoring}):
\begin{align*}
  \mathbf{f}(\mathbf{d}) & =
  \bigl(f_1(\mathbf{d}), f_2(\mathbf{d}), \ldots, f_{13}(\mathbf{d})\bigr)
\end{align*}

The current turn number can be represented as:
$$t \in \{1, 2, \ldots, 13\}, \quad t = \sum_{i=1}^{13} \mathbb{I}(c_i \neq \varnothing)$$

A single turn is composed of an initial dice roll, two optional re-rolls, and a final scoring decision.
Let $r = 0$, with $r \in \{0,1,2\}$ which is the number of rolls taken so far.
Prior to the first roll, the dice are randomized:
$$
  \mathbf{d}_{r=0} \sim U(\mathcal{D})
$$

The player must decide which dice to keep and which to re-roll. Let the player define a keep vector:
$$\mathbf{k} \in \{0,1\}^5$$
where $\mathbf{k}_i = 1$ indicates that die $i$ is kept, otherwise it is re-rolled.

We can then define the transition of the dice state after a re-roll as:
\begin{align*}
  \mathbf{d}' & \sim U(\mathcal{D}),                          \\[2pt]
  \mathbf{d}_{r+1}
              & = (\mathbf{1} - \mathbf{k}) \odot \mathbf{d}'
  + \mathbf{k} \odot \mathbf{d}
\end{align*}


When $r=2$, the player must choose a scoring category to apply their current dice to. Define a scoring choice mask as a one-hot vector:
$$\mathbf{s} \in \{0,1\}^{13}, \quad \|\mathbf{s}\|_1 = 1$$

For the purposes of calculating the final (or current) score, any field that has not been scored yet can be counted as zero.
We can define a mask vector for this:

\begin{align*}
   & \mathbf{u}(\mathbf{c}) \in \{0,1\}^{13}                                                                     \\
   & \mathbf{u}(\mathbf{c})_i = \mathbb{I}\bigl(c_i \neq \varnothing\bigr), \quad \forall i = \{ 1, \ldots 13 \}
\end{align*}


If a player achieves a total score of 63 or more in the upper section (categories 1-6), they receive a bonus of 35 points:
$$
  B(\mathbf{c}) = \begin{cases}
    0,  & 63 \leq \sum_{i=1}^{6} \mathbf{u}(\mathbf{c})_i \cdot \mathbf{c}_i \\
    35, & \text{otherwise}
  \end{cases}
$$

The player's score can thus be calculated as:
$$\mathrm{score}(\mathbf{c}) = B(\mathbf{c}) + \big\langle \mathbf{u}(\mathbf{c}), \mathbf{c} \big\rangle$$

\subsection{MDP Formulation}
We model \textit{Yahtzee} as a Markov Decision Process $(\mathcal S,\mathcal A,P,R,\gamma)$ \citep{Puterman1994MDP}.

A state is represented as $\mathbf{s} = (\mathbf{d},\mathbf{c},r, t)$, where $\mathbf{d}$ is the current
dice configuration, $\mathbf{c}$ the scorecard, and $r$ the roll index, and $t$ the current turn index
(see Section~\ref{sec:yahtzee-definitions}).

For simplicity, we define the action $\mathbf{a} = (\mathbf{k}, \mathbf{s})$, where $\mathbf{k}$ is the keep vector and $\mathbf{s}$ is the score category choice.
If $r < 2$, the $\mathbf{k}$ is used bt $P$, otherwise $\mathbf{s}$ is used.

The transition function $P$ is is specified in Appendix~\ref{app:transition-function}.

The reward is the change in total score between steps $R_t = \mathrm{score}(c_{t+1}) - \mathrm{score}(c_t)$,
and since we desire to maximize total score at the end of the game, we set $\gamma = 1$.


\subsection{Single-Turn Optimization Task}
\subsection{Full-Game Reinforcement Learning Task}

\section{Methodology}
1400-1700 words
\subsection{State Representation \& Input Features}
The design of $\mathbf{\phi}(\mathbf{s}) \rightarrow \mathbf{x}$ is one of the most critical components to the performance of a model \shortcite{sutton-2018-reinforcement-book}.

As such, several different representations were tested to evaluate their impact on learning efficiency and final performance.

\subsubsection{Dice Representation}
The dice representation can be encoded in several ways, depending on if we want to preserve permutation invariance or not.
Preserving ordering information (and implicitly, ranking) gives the model the benefit of being able to directly output actions corresponding to dice indices,
however, it comes at the cost of implicitly biasing the model to specific dice orderings; in other words, biasing towards a local optima of always keeping the highest ranking dice.
However, eliminating ordering information requires the model to either waste capacity learning permutation invariance or be architecturally supportive of it (e.g. with self-attention).
It also requires a different action representation, since actions can no longer correspond to specific dice indices. We attempted 5 different dice representations:
\begin{itemize}
  \item \textbf{One-hot encoding of ordered dice:} Each die is represented as a one-hot vector of length 6, and the 5 dice are concatenated in order.
  \item \textbf{Bin representation:} Here we pass through the dice face count vector $\mathbf{n}(\mathbf{d})$.
  \item \textbf{Combined representation:} Both ordered one-hot and bin representations are concatenated.
  \item \textbf{Learnable encoding of dice:} Each die is represented as a learnable embedding vector of length $d$, and self-attention is applied.
  \item \textbf{Learnable encoding of dice with positional encodings:} Similar to above, but with sinusoidal positional encodings added to each die embedding.
\end{itemize}

\subsubsection{Scorecard Representation}
There are two important pieces of information $\mathbf{\phi}$ must encode about the scorecard: whether a category is open or closed,
and some form of progress towards the upper bonus. For the category availability, we simply pass through $\mathbf{c}$. Here, several different representations were tested:
\begin{itemize}
  \item \textbf{Raw upper score:} $\sum_{i \in \mathbf{c}} c_{i}$
  \item \textbf{Distance to bonus:} $63 - \sum_{i \in \mathbf{c}} c_{i}$
  \item \textbf{Norm. upper score:} $\min(\frac{1}{63} \sum_{i \in \mathbf{c}} c_{i}, 1)$
  \item \textbf{Distance to bonus:} $\mathbb{I}(\sum_{i \in \mathbf{c}} c_{i} \geq 63)$
  \item \textbf{"Golf" score:}
\end{itemize}


\subsubsection{Computed Features}
\subsection{Neural Network Architecture}
\subsubsection{Trunk}
\subsubsection{Policy and Value Heads}
\subsubsection{Weight Initialization}
\subsubsection{Optimization \& Schedules}
\subsubsection{Training Metrics}
\subsection{Reinforcement Learning Algorithms}
\subsubsection{REINFORCE for Single-Turn Optimization}
\subsubsection{PPO for Full-Game Reinforcement Learning}
\subsection{Training Regimes}
\subsubsection{Single-Turn Training}
\subsubsection{Transfer Learning Setup}
\subsubsection{Evaluation Protocol}

\section{Results}

\subsection{Generalization of Single-Turn Agent to Full Game}
200-400 words

\subsection{Single-Turn Results \& Ablations}
600-700 words
\subsubsection{Baseline Model Performance}
\subsubsection{Representational Ablations}
\subsubsection{Architectural Ablations}
\subsubsection{Failure Mode Analysis}
\subsubsection{Summary}

\subsection{Full-Game Results}
600-700 words
\subsubsection{From-Scratch Training}
\subsubsection{REINFORCE}
\subsubsection{PPO}
\subsubsection{Transfer Learning}
\subsubsection{REINFORCE}
\subsubsection{PPO}
\subsubsection{Summary}

\subsection{Policy Analysis}
300-500 words
\subsubsection{Category Usage Distribution}
\subsubsection{Score Breakdown}
\subsubsection{Strategy Comparison}

\section{Discussion}
400-600 words

\section{Conclusion and Future Work}
200-300 words




\subsubsection{Neural Network Size Bounds}
One challenge in applying neural networks to approximate the value or policy function for Yahtzee is in
determining an appropriate network size. We will quickly consider theoretical upper and lower bounds for the number of neurons required using
both the full state space and a reduced representation.

Assume for simplicity our reduced representation consists of the dice state $\mathbf{d}$ encoded one-hot,
the roll count $r$ encoded one-hot,
the bin count vector $\mathbf{n}(\mathbf{d})$ encoded one-hot,
and a score mask $\mathbf{u}(\mathbf{c})$. This gives input dimensionality of $30 + 3 + 30 + 13 = 52$ and cardinality of
$2^{52} \approx 4.5 \times 10^{15}$.


For a discrete control problem with a finite state size of $|\mathbf{S}|$, there are classical results \cite{horne-1994-bounds-rnn-fsm}
showing that a recurrent neural network can represent any $|\mathbf{S}|$-state finite state machine with a number of neurons that
grows only logarithmically with $|\mathbf{S}|$. This gives us an upper bound of $O(\sqrt{|\mathbf{S}|}) \approx 8.4 \times 10^{7}$ neurons.
However, this is a prohibitively large number of neurons for practical training and inference.

For a lower bound, \citet{hanin-2017-bounded-width-relu} shows any continuous, convex function
can in principle be approximated by a network whose hidden layer width has a lower bound of $\Omega(d+1)$, and a non-convex function
can be approximated by a network with a hidden layer width of at least $\Omega(d+3)$, where $d$ is the input dimension.
For a example, assume Yahtzee is non-convex, and we represent the dice state $\mathbf{d}$ and $r$ as one-hot encodings and utilize
the score mask $\mathbf{u}(\mathbf{c})$, then $x = 6 \cdot 5 + 3 + 13 = 46$ means our \textit{minimum} hidden layer width is 49 neurons.

In practice, the ideal network size is not driven by the cardinality of the state space, but rather by the complexity of the function to be approximated, the richness of the data, and the specific requirements of the task at hand.

\bibliography{1_introduction,2_related_work,3_methods,4_single_turn_results,5_full_game_experiments,6_discussion,7_appendix}
\bibliographystyle{acl_natbib}

\appendix

\section{AI Usage}

This paper utilized artificial intelligence tools in the following ways:

\begin{itemize}
  \item \textbf{GitHub Copilot (Claude Sonnet 4.5)} was used for typesetting assistance with LaTeX/KaTeX and for autocomplete suggestions during coding.
  \item \textbf{ChatGPT (GPT-5)} was used for brainstorming ideas for reinforcement learning applications in games, guidance in hyperparameter tuning, helping to outline the structure of the paper, and suggesting relevant related work.
\end{itemize}

All other content, including research methodology, analysis, results interpretation, and conclusions, represents original work by the author. The AI tools were used only for initial ideation and to assist research, not for generating substantive content or analysis.

\section{Yahtzee Scoring Rules}
\label{app:scoring}
Next we define the indicator functions for each of the scoring categories:

\begin{align*}
  \mathbb{I}_{3\mathrm{k}}(\mathbf{d})
   & = \mathbb{I}\bigl\{ \max_{v} n_v(\mathbf{d}) \ge 3 \bigr\} \\[4pt]
  \mathbb{I}_{4\mathrm{k}}(\mathbf{d})
   & = \mathbb{I}\bigl\{ \max_{v} n_v(\mathbf{d}) \ge 4 \bigr\} \\[6pt]
  \mathbb{I}_{\mathrm{full}}(\mathbf{d})
   & = \mathbb{I}\Bigl\{
  \exists i, j \in \{1, \mathellipsis, 6 \} \ \text{with} \ n_i(\mathbf{d}) = 3 \land n_j(\mathbf{d}) = 2
  \Bigr\}                                                       \\[6pt]
  \mathbb{I}_{\mathrm{ss}}(\mathbf{d})
   & = \mathbb{I}\Bigl\{
  \exists k \in \{1,2,3\} \ \text{with}\
  \sum_{v=k}^{k+3} \mathbb{I}\{n_v(\mathbf{d}) > 0\} = 4
  \Bigr\}                                                       \\[6pt]
  \mathbb{I}_{\mathrm{ls}}(\mathbf{d})
   & = \mathbb{I}\Bigl\{
  \exists k \in \{1,2\} \ \text{with}\
  \sum_{v=k}^{k+4} \mathbb{I}\{n_v(\mathbf{d}) > 0\} = 5
  \Bigr\}                                                       \\[6pt]
  \mathbb{I}_{\mathrm{yahtzee}}(\mathbf{d})
   & = \mathbb{I}\bigl\{\max_v n_v(\mathbf{d}) = 5\bigr\}
\end{align*}

The potential score for each category can then be defined as:
\begin{align*}
  f_j(\mathbf{d})        & = j \cdot n_j(\mathbf{d}), \qquad j \in \{1,\dots,6\}                   \\[4pt]
  f_7(\mathbf{d})        & = \mathbf{1}^\top \mathbf{d} \cdot \mathbb{I}_{3\mathrm{k}}(\mathbf{d}) \\[4pt]
  f_8(\mathbf{d})        & = \mathbf{1}^\top \mathbf{d} \cdot \mathbb{I}_{4\mathrm{k}}(\mathbf{d}) \\[4pt]
  f_9(\mathbf{d})        & = 25 \cdot \mathbb{I}_{\mathrm{full}}(\mathbf{d})                       \\[4pt]
  f_{10}(\mathbf{d})     & = 30 \cdot \mathbb{I}_{\mathrm{ss}}(\mathbf{d})                         \\[4pt]
  f_{11}(\mathbf{d})     & = 40 \cdot \mathbb{I}_{\mathrm{ls}}(\mathbf{d})                         \\[4pt]
  f_{12}(\mathbf{d})     & = 50 \cdot \mathbb{I}_{\mathrm{yahtzee}}(\mathbf{d})                    \\[4pt]
  f_{13}(\mathbf{d})     & = \mathbf{1}^\top \cdot \mathbf{d}                                      \\[4pt]
  \mathbf{f}(\mathbf{d}) & =
  \bigl(f_1(\mathbf{d}), f_2(\mathbf{d}), \ldots, f_{13}(\mathbf{d})\bigr)
\end{align*}

\section{State Transition Function}
\label{app:transition-function}

$P$ can be defined by the following generative process.

\begin{itemize}
  \item If $r < 2$ and $a = k$, for each die $i$:
        \begin{itemize}
          \item if $k_i = 1$, keep $d'_i = d_i$;
          \item else sample $d'_i \sim \mathrm{Unif}\{1,\dots,6\}$ independently.
        \end{itemize}
        Set $c' = c,\ r' = r+1,\ t' = t$.
  \item If $r = 2$ and $a = i$, set $d' = d$, update $c' = \mathrm{score}(c,d,i)$,
        set $r' = 0,\ t' = t+1$.
\end{itemize}


\end{document}
