% ============================================================================
% 5. FULL-GAME EXPERIMENTS (Transfer vs. Scratch, Algo Comparisons)
% ============================================================================
% Again, no big new references, but several re-uses:
%
% When talking about transfer vs learning from scratch in the context of games:
%   - silver2017alphagozero (defined in 1_introduction.bib)
%   - silver2018alphazero (defined in 1_introduction.bib)
%   Example: "Unlike AlphaGo Zero, which learns purely from self-play, we explore 
%            a hybrid approach that pretrains on single-turn tasks before full-game training."
%
% When justifying PPO vs REINFORCE / A2C:
%   - schulman-2017-ppo (defined in 2_related_work.bib)
%   - mnih-2016-a3c (defined in 2_related_work.bib)
%   - sutton-2000-policy-gradient (defined in 2_related_work.bib)
%
% When discussing credit assignment and instability in full-game scores:
%   - tesauro1992practicaltd (defined in 2_related_work.bib)
%   - bjorck-2022-high-variance (defined in 2_related_work.bib)
%   - lyle-2024-normalization-rl (defined in 3_methods.bib)
%
% If you explicitly mention exploration issues in the full game (e.g., rare Yahtzees / long tails):
%   - osband-2016-bootstrappeddqn (defined in 1_introduction.bib)
%
% No new references needed for this section - just back-reference to earlier papers.
